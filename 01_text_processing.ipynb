{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d846cb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03838406",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.1 Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79995afa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* We will analyze a very well known NLP dataset: tweets from disaster\n",
    "\n",
    "\n",
    "* It is a Kaggle competition, which offers a simple but good level textual dataset to be able to make its weapons in NLP\n",
    "\n",
    "\n",
    "* The dataset is here [https://www.kaggle.com/competitions/nlp-getting-started/data]\n",
    "\n",
    "\n",
    "* Please use the **train** dataset\n",
    "\n",
    "\n",
    "* In this 1st part we are going to clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562f101",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.2 Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885639a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:47:48.424652Z",
     "start_time": "2022-06-28T20:47:48.419964Z"
    },
    "hidden": true
   },
   "source": [
    "You have to install  : \n",
    "\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* seaborn\n",
    "\n",
    "\n",
    "* nltk\n",
    "* wordcloud\n",
    "* pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97396f16",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.3 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d56dbd5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.710893Z",
     "start_time": "2022-06-30T16:04:40.704506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, sys, time, random\n",
    "\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# import spacy\n",
    "\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# import plotly as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a272cd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.4 Downloads and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8f872b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.721932Z",
     "start_time": "2022-06-30T16:04:40.713522Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/alex/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download\n",
    "\n",
    "\"\"\"\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ecb0d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.727314Z",
     "start_time": "2022-06-30T16:04:40.724139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "765b4148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.731742Z",
     "start_time": "2022-06-30T16:04:40.729554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init pandarallel\n",
    "\n",
    "# pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880a917",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1.5 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c046fdbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.737695Z",
     "start_time": "2022-06-30T16:04:40.733511Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['min_10_words.csv',\n",
       " 'df.csv',\n",
       " 'final_df.csv',\n",
       " 'df_cleaned.csv',\n",
       " 'unique_words.csv',\n",
       " 'finad_df.csv',\n",
       " 'min_5_words.csv']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our file\n",
    "\n",
    "data = \"./data/cleaned/\"\n",
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e699e715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.758553Z",
     "start_time": "2022-06-30T16:04:40.739506Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataframe\n",
    "\n",
    "fn = data + 'df_cleaned.csv'\n",
    "df = pd.read_csv(fn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab788d1d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dea19425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.765238Z",
     "start_time": "2022-06-30T16:04:40.760515Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@holymileyray @moonIighthunty Focus on Me is going to obliterate careers tea'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a random document\n",
    "\n",
    "doc = df.text.sample(1)\n",
    "doc = doc.values[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86b385",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1754801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.771570Z",
     "start_time": "2022-06-30T16:04:40.767345Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@holymileyray @mooniighthunty focus on me is going to obliterate careers tea'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower\n",
    "\n",
    "doc = doc.lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac2ec5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05a5c20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.778284Z",
     "start_time": "2022-06-30T16:04:40.774484Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'holymileyray',\n",
       " '@',\n",
       " 'mooniighthunty',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'me',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'obliterate',\n",
       " 'careers',\n",
       " 'tea']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "\n",
    "tokens = word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09531ad9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dab7ca33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.783781Z",
     "start_time": "2022-06-30T16:04:40.780299Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc474bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.787967Z",
     "start_time": "2022-06-30T16:08:54.787945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_tokens_info(tokens) : \n",
    "    \"\"\"display info about corpus \"\"\"\n",
    "    \n",
    "    print(f\"taille corpus {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f6a396b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.794352Z",
     "start_time": "2022-06-30T16:04:40.790576Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'holymileyray',\n",
       " '@',\n",
       " 'mooniighthunty',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'me',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'obliterate',\n",
       " 'careers',\n",
       " 'tea']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an other tokenize\n",
    "\n",
    "tokens = wordpunct_tokenize(doc)\n",
    "display_tokens_info(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dee43c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a6ec4db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.811669Z",
     "start_time": "2022-06-30T16:04:40.807001Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hasn', \"shan't\", 'won', 'they', 'me', 'both', \"haven't\", 'them', 'at', 'of', 'did', \"aren't\", 'by', 'mightn', 'does', 'o', \"wasn't\", 'mustn', \"shouldn't\", 'some', 'wouldn', 'before', 'not', 'against', 'through', 'nor', 'i', 'other', 'didn', 'and', 'own', 'themselves', 'for', 'd', \"she's\", \"doesn't\", 'her', 'from', 'off', 'my', 'hers', 'have', 'between', \"didn't\", 'his', 'itself', 've', 'you', 'that', 'why', 'she', 'further', 'aren', \"mustn't\", 'been', 'there', 'hadn', 'whom', 'under', 'shouldn', 'again', 'same', 'few', 'all', 'if', 'should', 'ours', 'has', \"weren't\", 're', 'a', \"won't\", 'into', 'most', 'do', 'll', 'can', \"don't\", 'until', \"needn't\", \"wouldn't\", \"hasn't\", 'in', \"you're\", 'doing', 'are', 'or', 'don', 'am', 'being', 'those', 'no', 'only', 'isn', 'm', 'herself', 'as', 'with', 'its', 'their', 'your', \"it's\", 'here', 'any', 'after', 'yourselves', 'above', 'what', 'where', 'up', 'needn', 'our', 'out', 'yourself', 'how', 'will', 'during', 'it', 'because', 'ma', 'be', 'over', 'wasn', 't', 'which', \"isn't\", \"mightn't\", 'were', 'doesn', 'shan', 'when', \"that'll\", 'ourselves', 'y', 'just', \"couldn't\", 'ain', 'down', 'these', 'each', 'but', 'than', 'haven', 'had', \"you'd\", 'then', 's', \"hadn't\", \"you'll\", 'too', 'below', 'is', 'so', 'about', 'having', 'on', 'who', 'theirs', 'while', 'to', 'once', 'an', 'the', 'this', \"you've\", 'he', 'more', 'such', 'yours', 'was', 'myself', 'now', 'very', 'weren', \"should've\", 'couldn', 'we', 'him', 'himself'}\n"
     ]
    }
   ],
   "source": [
    "# stop_words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4792ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.818946Z",
     "start_time": "2022-06-30T16:04:40.813931Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'holymileyray',\n",
       " '@',\n",
       " 'mooniighthunty',\n",
       " 'focus',\n",
       " 'going',\n",
       " 'obliterate',\n",
       " 'careers',\n",
       " 'tea']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_info(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1eda51a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.840049Z",
     "start_time": "2022-06-30T16:04:40.834578Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holymileyray',\n",
       " 'mooniighthunty',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'me',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'obliterate',\n",
       " 'careers',\n",
       " 'tea']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an other tokensizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "display_tokens_info(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba57384b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.866431Z",
     "start_time": "2022-06-30T16:04:40.861661Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holymileyray',\n",
       " 'mooniighthunty',\n",
       " 'focus',\n",
       " 'going',\n",
       " 'obliterate',\n",
       " 'careers',\n",
       " 'tea']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_info(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494c560",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.4 First cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fbd3174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.888066Z",
     "start_time": "2022-06-30T16:04:40.883040Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_text_1(doc, rejoin=False) : \n",
    "    \"\"\"basic function of text processing \"\"\"\n",
    "    \n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    if rejoin : \n",
    "        return \" \".join(cleaned_tokens_list)\n",
    "    \n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9eafd633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.895603Z",
     "start_time": "2022-06-30T16:04:40.890493Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holymileyray',\n",
       " 'mooniighthunty',\n",
       " 'focus',\n",
       " 'going',\n",
       " 'obliterate',\n",
       " 'careers',\n",
       " 'tea']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = process_text_1(doc)\n",
    "display_tokens_info(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdea23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Working on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66489ad6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.1 Build raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "047ce4ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:40.903124Z",
     "start_time": "2022-06-30T16:04:40.897752Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us allForest fire near La Ronge Sask. CanadaAll residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected13,000 people receive #wildfires evacuation orders in California Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areasI'm on top of the hill and I can see a fire in the woods...There's an emergency evacuation happening now in the building across the streetI'm afraid that the tornado is coming to our area...Three people died from the heat wave so farHaha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding#raining #flooding #Florida #TampaBay #T\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join all corpus\n",
    "\n",
    "raw_corpus = \"\".join(df.text.values)\n",
    "raw_corpus[:1_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af82dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.785553Z",
     "start_time": "2022-06-30T16:08:54.785535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# process the corpus\n",
    "\n",
    "corpus = process_text_1(raw_corpus)\n",
    "display_tokens_info(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3202abe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.039515Z",
     "start_time": "2022-06-30T16:04:41.004810Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co                  4703\n",
       "http                4231\n",
       "https                405\n",
       "amp                  342\n",
       "like                 341\n",
       "                    ... \n",
       "destructiontruck       1\n",
       "salvages               1\n",
       "7b2wf6ovfk             1\n",
       "newsrepublican         1\n",
       "ymy4rskq3d             1\n",
       "Length: 22438, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value counts\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a19ff84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.044801Z",
     "start_time": "2022-06-30T16:04:41.041859Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "# sns.barplot(x=tmp.index, y=tmp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33e385b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.056871Z",
     "start_time": "2022-06-30T16:04:41.047889Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co            4703\n",
       "http          4231\n",
       "https          405\n",
       "amp            342\n",
       "like           341\n",
       "û_             289\n",
       "fire           230\n",
       "get            226\n",
       "via            216\n",
       "2              204\n",
       "people         189\n",
       "new            183\n",
       "one            181\n",
       "news           166\n",
       "emergency      145\n",
       "disaster       143\n",
       "video          136\n",
       "would          133\n",
       "body           127\n",
       "police         122\n",
       "still          120\n",
       "3              119\n",
       "u              117\n",
       "crash          117\n",
       "us             115\n",
       "storm          114\n",
       "back           113\n",
       "day            112\n",
       "know           112\n",
       "california     110\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30st most common tokens\n",
    "\n",
    "tmp.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41d809e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.065128Z",
     "start_time": "2022-06-30T16:04:41.059511Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tydxauuenqhow          1\n",
       "developer              1\n",
       "hld5xlywbncrackdown    1\n",
       "lmwkjsycgj             1\n",
       "danhrothschild         1\n",
       "greed                  1\n",
       "takecare               1\n",
       "cinla1964              1\n",
       "windowgatribble        1\n",
       "contrasts              1\n",
       "foreboding             1\n",
       "expansive              1\n",
       "divisions              1\n",
       "saturation             1\n",
       "hue                    1\n",
       "qbmcsjavt0fall         1\n",
       "homebuyer              1\n",
       "miscalculation         1\n",
       "mwjcdkthere            1\n",
       "workspace              1\n",
       "forsee                 1\n",
       "badkitty               1\n",
       "lt3dave                1\n",
       "specs                  1\n",
       "lore                   1\n",
       "destructiontruck       1\n",
       "salvages               1\n",
       "7b2wf6ovfk             1\n",
       "newsrepublican         1\n",
       "ymy4rskq3d             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30st last common tokens\n",
    "\n",
    "tmp.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "547c0079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.078036Z",
     "start_time": "2022-06-30T16:04:41.067702Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22438.000000\n",
       "mean         3.680096\n",
       "std         43.379216\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          2.000000\n",
       "max       4703.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d81a9a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.084635Z",
     "start_time": "2022-06-30T16:04:41.081243Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sns.displot(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c5c76df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.090905Z",
     "start_time": "2022-06-30T16:04:41.087822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sns.boxplot(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15905e9c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.2 List rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaca7018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.126660Z",
     "start_time": "2022-06-30T16:04:41.093829Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dub                   1\n",
       "mxhrextrkh            1\n",
       "ctijdpxabkdogs        1\n",
       "splatling             1\n",
       "foothill              1\n",
       "designsso             1\n",
       "thatrussianman        1\n",
       "waterboarding         1\n",
       "writingtips           1\n",
       "salmanmydarling       1\n",
       "ps3                   1\n",
       "xboxhttps             1\n",
       "qr1l2jyuez            1\n",
       "nester                1\n",
       "switching             1\n",
       "dipping               1\n",
       "pantherattackthere    1\n",
       "dieanpink95           1\n",
       "limitsabe             1\n",
       "yu_nita99             1\n",
       "sivan                 1\n",
       "pantherattacki        1\n",
       "camilla_33            1\n",
       "uooygbb6az            1\n",
       "akq4rwjfvlcheck       1\n",
       "skippy6gaming         1\n",
       "slttorrlhswho         1\n",
       "craykain              1\n",
       "lavalet               1\n",
       "basalt                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words --> not usefull\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_unique_words = tmp[tmp==1]\n",
    "list_unique_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c7ccc02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.136930Z",
     "start_time": "2022-06-30T16:04:41.129595Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16230"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2eb9b8bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.150309Z",
     "start_time": "2022-06-30T16:04:41.140172Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dub',\n",
       " 'mxhrextrkh',\n",
       " 'ctijdpxabkdogs',\n",
       " 'splatling',\n",
       " 'foothill',\n",
       " 'designsso',\n",
       " 'thatrussianman',\n",
       " 'waterboarding',\n",
       " 'writingtips',\n",
       " 'salmanmydarling',\n",
       " 'ps3',\n",
       " 'xboxhttps',\n",
       " 'qr1l2jyuez',\n",
       " 'nester',\n",
       " 'switching',\n",
       " 'dipping',\n",
       " 'pantherattackthere',\n",
       " 'dieanpink95',\n",
       " 'limitsabe',\n",
       " 'yu_nita99',\n",
       " 'sivan',\n",
       " 'pantherattacki',\n",
       " 'camilla_33',\n",
       " 'uooygbb6az',\n",
       " 'akq4rwjfvlcheck',\n",
       " 'skippy6gaming',\n",
       " 'slttorrlhswho',\n",
       " 'craykain',\n",
       " 'lavalet',\n",
       " 'basalt']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words = list(list_unique_words.index)\n",
    "list_unique_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b76a7568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.186465Z",
     "start_time": "2022-06-30T16:04:41.152747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save it for later\n",
    "\n",
    "tmp = pd.DataFrame({\"words\" : list_unique_words})\n",
    "tmp.to_csv(\"data/cleaned/unique_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07325ab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.223581Z",
     "start_time": "2022-06-30T16:04:41.189164Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "motorcycle     5\n",
       "blind          5\n",
       "ices           5\n",
       "remain         5\n",
       "md             5\n",
       "mental         5\n",
       "loves          5\n",
       "depth          5\n",
       "extra          5\n",
       "leaves         5\n",
       "subs           5\n",
       "judge          5\n",
       "earners        5\n",
       "operations     5\n",
       "reduced        5\n",
       "catch          5\n",
       "stephen        5\n",
       "quest          5\n",
       "reviews        5\n",
       "responsible    5\n",
       "motor          5\n",
       "flying         5\n",
       "smithsonian    5\n",
       "52             5\n",
       "34             5\n",
       "losses         5\n",
       "desires        5\n",
       "pulls          5\n",
       "mood           5\n",
       "tubestrike     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idem for min 5 times\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_5_words = tmp[tmp<=5]\n",
    "list_min_5_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f8e6b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.231803Z",
     "start_time": "2022-06-30T16:04:41.226360Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20275"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9fc67",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save it \n",
    "\n",
    "list_min_5_words = list(list_min_5_words.index)\n",
    "tmp = pd.DataFrame({\"words\" : list_min_10_words})\n",
    "tmp.to_csv(\"data/cleaned/min_5_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e8600ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.270885Z",
     "start_time": "2022-06-30T16:04:41.234997Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "able           10\n",
       "trees          10\n",
       "complete       10\n",
       "udhampur       10\n",
       "seattle        10\n",
       "word           10\n",
       "michael        10\n",
       "yyc            10\n",
       "amazon         10\n",
       "grows          10\n",
       "jeb            10\n",
       "afghanistan    10\n",
       "picture        10\n",
       "abandoned      10\n",
       "ice            10\n",
       "main           10\n",
       "emotional      10\n",
       "sit            10\n",
       "colour         10\n",
       "nice           10\n",
       "tent           10\n",
       "extreme        10\n",
       "lmao           10\n",
       "ii             10\n",
       "loved          10\n",
       "seeks          10\n",
       "extremely      10\n",
       "issue          10\n",
       "either         10\n",
       "incident       10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idem for min 10 times\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_10_words = tmp[tmp<=10]\n",
    "list_min_10_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddcd474b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.278055Z",
     "start_time": "2022-06-30T16:04:41.273172Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21158"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7328e285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.332741Z",
     "start_time": "2022-06-30T16:04:41.280639Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save it \n",
    "\n",
    "list_min_10_words = list(list_min_10_words.index)\n",
    "tmp = pd.DataFrame({\"words\" : list_min_10_words})\n",
    "tmp.to_csv(\"data/cleaned/min_10_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f9a94",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.3 2nd Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fb47469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.343989Z",
     "start_time": "2022-06-30T16:04:41.335687Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_text_2(doc, \n",
    "                   rejoin=False, \n",
    "                   list_rare_words=None, \n",
    "                   min_len_word=3,\n",
    "                   force_is_alpha=True) : \n",
    "    \"\"\"cf process_text_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "    \n",
    "    positional arguments : \n",
    "    -----------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "    \n",
    "    opt args : \n",
    "    -----------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : the minimum length of words to not exclude\n",
    "    force_is_alpha : int : if 1, exclude all tokens with a numeric character\n",
    "    \n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # classics stopwords\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    \n",
    "    ###########################################################\n",
    "    ###########################################################\n",
    "    \n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # no more len words\n",
    "    more_than_N =  [w for w in non_rare_tokens if len(w) >= min_len_word  ]\n",
    "    \n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else :\n",
    "        alpha_tokens = more_than_N\n",
    "        \n",
    "    ###########################################################\n",
    "    ###########################################################     \n",
    "    \n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "205a7e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.362983Z",
     "start_time": "2022-06-30T16:04:41.346666Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8228            watched\n",
       "76087                co\n",
       "43740                 w\n",
       "17831              hope\n",
       "46693             heavy\n",
       "49410                20\n",
       "34516    rebeccaforreal\n",
       "60426           content\n",
       "14544              come\n",
       "66576                3g\n",
       "31278              days\n",
       "49233          wireless\n",
       "60798            toward\n",
       "10628              spot\n",
       "46025        skdbot7tgf\n",
       "56456                co\n",
       "58884             going\n",
       "54144               gun\n",
       "17824             munch\n",
       "31182               fan\n",
       "6224               http\n",
       "49474         hurricane\n",
       "8590        wealilknowa\n",
       "71937             saudi\n",
       "22098              goku\n",
       "57098        auntiedote\n",
       "199              ablaze\n",
       "55878              http\n",
       "57798                co\n",
       "67625              june\n",
       "dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(corpus).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f08cac1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:04:41.377543Z",
     "start_time": "2022-06-30T16:04:41.366190Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22438"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0441adca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:04.294324Z",
     "start_time": "2022-06-30T16:04:41.380391Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49348     hiroshima\n",
       "42725          http\n",
       "22656         every\n",
       "52520           may\n",
       "38232         plant\n",
       "43245         would\n",
       "37466      disaster\n",
       "5492           blew\n",
       "45691       another\n",
       "6235           done\n",
       "46257          know\n",
       "34193        dunbar\n",
       "43743          ruin\n",
       "51061        summer\n",
       "33625    government\n",
       "13314         hills\n",
       "23569         thurs\n",
       "44754          http\n",
       "44020           get\n",
       "45517          haha\n",
       "52130      declares\n",
       "11669         china\n",
       "22503      freezing\n",
       "39113         times\n",
       "18996      occurred\n",
       "14896          soon\n",
       "46312          time\n",
       "31147     dangerous\n",
       "55025        killed\n",
       "14514          http\n",
       "dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_text_2(raw_corpus, list_rare_words=list_unique_words, rejoin=False)\n",
    "pd.Series(corpus).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0d204c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:04.306895Z",
     "start_time": "2022-06-30T16:05:04.297047Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5705"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d441b7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.4 Stem and Lem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a4739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae954a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b793577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:04.315368Z",
     "start_time": "2022-06-30T16:05:04.309410Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = \"I have 3 dogs, they was all black. Now they are all white but one of my dog is my favorite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f59967f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:04.323152Z",
     "start_time": "2022-06-30T16:05:04.318562Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', '3', 'dogs', 'they', 'was', 'all', 'black', 'now', 'they', 'are', 'all', 'white', 'but', 'one', 'of', 'my', 'dog', 'is', 'my', 'favorite']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(doc.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "47884fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:04.337249Z",
     "start_time": "2022-06-30T16:05:04.332843Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', '3', 'dog', 'they', 'wa', 'all', 'black', 'now', 'they', 'are', 'all', 'white', 'but', 'one', 'of', 'my', 'dog', 'is', 'my', 'favorit']\n"
     ]
    }
   ],
   "source": [
    "trans = PorterStemmer()\n",
    "trans_text = [trans.stem(i) for i in tokens ]\n",
    "print(trans_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29079eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:05.872034Z",
     "start_time": "2022-06-30T16:05:04.339536Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', '3', 'dog', 'they', 'wa', 'all', 'black', 'now', 'they', 'are', 'all', 'white', 'but', 'one', 'of', 'my', 'dog', 'is', 'my', 'favorite']\n"
     ]
    }
   ],
   "source": [
    "trans = WordNetLemmatizer()\n",
    "trans_text = [trans.lemmatize(i) for i in tokens ]\n",
    "print(trans_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b12375",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.5 3rd cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "906a48b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:05.882327Z",
     "start_time": "2022-06-30T16:05:05.874557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_text_3(doc, \n",
    "                   rejoin=False, \n",
    "                   lemm_or_stemm=\"stem\",\n",
    "                   list_rare_words=None, \n",
    "                   min_len_word=3,\n",
    "                   force_is_alpha=True) : \n",
    "    \"\"\"cf process_text_2 but with stemm or lem\n",
    "    \n",
    "    positional arguments : \n",
    "    -----------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "    \n",
    "    opt args : \n",
    "    -----------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    lemm_or_stemm : str : if lem do lemmentize else stemmentize  \n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : the minimum length of words to not exclude\n",
    "    force_is_alpha : int : if 1, exclude all tokens with a numeric character\n",
    "    \n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # classics stopwords\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # no more len words\n",
    "    more_than_N =  [w for w in non_rare_tokens if len(w) >= min_len_word  ]\n",
    "    \n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else :\n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    ###########################################################\n",
    "    ###########################################################\n",
    "    \n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_tokens ]\n",
    "    else : \n",
    "        trans = PorterStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens ]\n",
    "        \n",
    "     ###########################################################\n",
    "     ###########################################################\n",
    "    \n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(trans_text)\n",
    "    \n",
    "    return trans_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7400fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:30.570673Z",
     "start_time": "2022-06-30T16:05:05.884795Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41284           updat\n",
       "8440             http\n",
       "10676            work\n",
       "47264          turkey\n",
       "40523          bounti\n",
       "33228         florida\n",
       "21271           creat\n",
       "12937            work\n",
       "36215           white\n",
       "25981            ebay\n",
       "31599             via\n",
       "34891        restrict\n",
       "35723           heard\n",
       "51664            like\n",
       "34420             add\n",
       "14925            yall\n",
       "52096    nasahurrican\n",
       "12233         collaps\n",
       "13166           learn\n",
       "40377          gunman\n",
       "48868            tree\n",
       "8169             drop\n",
       "55718           debat\n",
       "41030            http\n",
       "7457             bodi\n",
       "26594           crash\n",
       "22540      electrocut\n",
       "18472        destruct\n",
       "44370           shell\n",
       "41974       entertain\n",
       "dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_text_3(raw_corpus, rejoin=False, list_rare_words=list_unique_words)\n",
    "pd.Series(corpus).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "28768381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:30.582927Z",
     "start_time": "2022-06-30T16:05:30.573124Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4420"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c8be691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:30.750270Z",
     "start_time": "2022-06-30T16:05:30.585665Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96512        interneciary\n",
       "122872             neanic\n",
       "36522        circularizer\n",
       "133393            outcase\n",
       "86589          Holodiscus\n",
       "131767       orbicularity\n",
       "24068           boarspear\n",
       "230268             wallop\n",
       "87285             hookman\n",
       "47757         cypressroot\n",
       "184913    somnambulically\n",
       "66652              exodos\n",
       "187461           sporange\n",
       "216458         unfendered\n",
       "214487          underpile\n",
       "221689     unremunerating\n",
       "22370             biltong\n",
       "119952           mottling\n",
       "96494              intern\n",
       "75782       gastrostomize\n",
       "65951             evestar\n",
       "37720          clodhopper\n",
       "188432        stalactitic\n",
       "198029        tangleberry\n",
       "228853         Vineyarder\n",
       "234729          yohimbine\n",
       "16889            axometry\n",
       "181024           sickling\n",
       "85818        Hildebrandic\n",
       "187631           spraggly\n",
       "dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series( words.words()).sample(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f275fb7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.5 Only english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a7511e15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:30.897500Z",
     "start_time": "2022-06-30T16:05:30.753731Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235892"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6572bab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:31.043562Z",
     "start_time": "2022-06-30T16:05:30.900522Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'aaron',\n",
       " 'aaronic',\n",
       " 'aaronical',\n",
       " 'aaronite',\n",
       " 'aaronitic',\n",
       " 'aaru',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'ababdeh',\n",
       " 'ababua',\n",
       " 'abac',\n",
       " 'abaca',\n",
       " 'abacate',\n",
       " 'abacay',\n",
       " 'abacinate',\n",
       " 'abacination',\n",
       " 'abaciscus',\n",
       " 'abacist',\n",
       " 'aback',\n",
       " 'abactinal',\n",
       " 'abactinally',\n",
       " 'abaction',\n",
       " 'abactor',\n",
       " 'abaculus',\n",
       " 'abacus',\n",
       " 'abadite',\n",
       " 'abaff',\n",
       " 'abaft',\n",
       " 'abaisance',\n",
       " 'abaiser',\n",
       " 'abaissed',\n",
       " 'abalienate',\n",
       " 'abalienation',\n",
       " 'abalone',\n",
       " 'abama',\n",
       " 'abampere',\n",
       " 'abandon',\n",
       " 'abandonable',\n",
       " 'abandoned',\n",
       " 'abandonedly',\n",
       " 'abandonee',\n",
       " 'abandoner',\n",
       " 'abandonment',\n",
       " 'abanic',\n",
       " 'abantes',\n",
       " 'abaptiston',\n",
       " 'abarambo',\n",
       " 'abaris',\n",
       " 'abarthrosis',\n",
       " 'abarticular',\n",
       " 'abarticulation',\n",
       " 'abas',\n",
       " 'abase',\n",
       " 'abased',\n",
       " 'abasedly',\n",
       " 'abasedness',\n",
       " 'abasement',\n",
       " 'abaser',\n",
       " 'abasgi',\n",
       " 'abash',\n",
       " 'abashed',\n",
       " 'abashedly',\n",
       " 'abashedness',\n",
       " 'abashless',\n",
       " 'abashlessly',\n",
       " 'abashment',\n",
       " 'abasia',\n",
       " 'abasic',\n",
       " 'abask',\n",
       " 'abassin',\n",
       " 'abastardize',\n",
       " 'abatable',\n",
       " 'abate',\n",
       " 'abatement',\n",
       " 'abater',\n",
       " 'abatis',\n",
       " 'abatised',\n",
       " 'abaton',\n",
       " 'abator',\n",
       " 'abattoir',\n",
       " 'abatua',\n",
       " 'abature',\n",
       " 'abave',\n",
       " 'abaxial',\n",
       " 'abaxile',\n",
       " 'abaze',\n",
       " 'abb',\n",
       " 'abba',\n",
       " 'abbacomes',\n",
       " 'abbacy',\n",
       " 'abbadide',\n",
       " 'abbas',\n",
       " 'abbasi',\n",
       " 'abbassi',\n",
       " 'abbasside',\n",
       " 'abbatial',\n",
       " 'abbatical',\n",
       " 'abbess',\n",
       " 'abbey',\n",
       " 'abbeystede',\n",
       " 'abbie',\n",
       " 'abbot',\n",
       " 'abbotcy',\n",
       " 'abbotnullius',\n",
       " 'abbotship',\n",
       " 'abbreviate',\n",
       " 'abbreviately',\n",
       " 'abbreviation',\n",
       " 'abbreviator',\n",
       " 'abbreviatory',\n",
       " 'abbreviature',\n",
       " 'abby',\n",
       " 'abcoulomb',\n",
       " 'abdal',\n",
       " 'abdat',\n",
       " 'abderian',\n",
       " 'abderite',\n",
       " 'abdest',\n",
       " 'abdicable',\n",
       " 'abdicant',\n",
       " 'abdicate',\n",
       " 'abdication',\n",
       " 'abdicative',\n",
       " 'abdicator',\n",
       " 'abdiel',\n",
       " 'abditive',\n",
       " 'abditory',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abdominales',\n",
       " 'abdominalian',\n",
       " 'abdominally',\n",
       " 'abdominoanterior',\n",
       " 'abdominocardiac',\n",
       " 'abdominocentesis',\n",
       " 'abdominocystic',\n",
       " 'abdominogenital',\n",
       " 'abdominohysterectomy',\n",
       " 'abdominohysterotomy',\n",
       " 'abdominoposterior',\n",
       " 'abdominoscope',\n",
       " 'abdominoscopy',\n",
       " 'abdominothoracic',\n",
       " 'abdominous',\n",
       " 'abdominovaginal',\n",
       " 'abdominovesical',\n",
       " 'abduce',\n",
       " 'abducens',\n",
       " 'abducent',\n",
       " 'abduct',\n",
       " 'abduction',\n",
       " 'abductor',\n",
       " 'abe',\n",
       " 'abeam',\n",
       " 'abear',\n",
       " 'abearance',\n",
       " 'abecedarian',\n",
       " 'abecedarium',\n",
       " 'abecedary',\n",
       " 'abed',\n",
       " 'abeigh',\n",
       " 'abel',\n",
       " 'abele',\n",
       " 'abelia',\n",
       " 'abelian',\n",
       " 'abelicea',\n",
       " 'abelite',\n",
       " 'abelite',\n",
       " 'abelmoschus',\n",
       " 'abelmosk',\n",
       " 'abelonian',\n",
       " 'abeltree',\n",
       " 'abencerrages',\n",
       " 'abenteric',\n",
       " 'abepithymia',\n",
       " 'aberdeen',\n",
       " 'aberdevine',\n",
       " 'aberdonian',\n",
       " 'aberia',\n",
       " 'aberrance',\n",
       " 'aberrancy',\n",
       " 'aberrant',\n",
       " 'aberrate',\n",
       " 'aberration',\n",
       " 'aberrational',\n",
       " 'aberrator',\n",
       " 'aberrometer',\n",
       " 'aberroscope',\n",
       " 'aberuncator',\n",
       " 'abet',\n",
       " 'abetment',\n",
       " 'abettal',\n",
       " 'abettor',\n",
       " 'abevacuation',\n",
       " 'abey',\n",
       " 'abeyance',\n",
       " 'abeyancy',\n",
       " 'abeyant',\n",
       " 'abfarad',\n",
       " 'abhenry',\n",
       " 'abhiseka',\n",
       " 'abhominable',\n",
       " 'abhor',\n",
       " 'abhorrence',\n",
       " 'abhorrency',\n",
       " 'abhorrent',\n",
       " 'abhorrently',\n",
       " 'abhorrer',\n",
       " 'abhorrible',\n",
       " 'abhorring',\n",
       " 'abhorson',\n",
       " 'abidal',\n",
       " 'abidance',\n",
       " 'abide',\n",
       " 'abider',\n",
       " 'abidi',\n",
       " 'abiding',\n",
       " 'abidingly',\n",
       " 'abidingness',\n",
       " 'abie',\n",
       " 'abies',\n",
       " 'abietate',\n",
       " 'abietene',\n",
       " 'abietic',\n",
       " 'abietin',\n",
       " 'abietineae',\n",
       " 'abietineous',\n",
       " 'abietinic',\n",
       " 'abiezer',\n",
       " 'abigail',\n",
       " 'abigail',\n",
       " 'abigailship',\n",
       " 'abigeat',\n",
       " 'abigeus',\n",
       " 'abilao',\n",
       " 'ability',\n",
       " 'abilla',\n",
       " 'abilo',\n",
       " 'abintestate',\n",
       " 'abiogenesis',\n",
       " 'abiogenesist',\n",
       " 'abiogenetic',\n",
       " 'abiogenetical',\n",
       " 'abiogenetically',\n",
       " 'abiogenist',\n",
       " 'abiogenous',\n",
       " 'abiogeny',\n",
       " 'abiological',\n",
       " 'abiologically',\n",
       " 'abiology',\n",
       " 'abiosis',\n",
       " 'abiotic',\n",
       " 'abiotrophic',\n",
       " 'abiotrophy',\n",
       " 'abipon',\n",
       " 'abir',\n",
       " 'abirritant',\n",
       " 'abirritate',\n",
       " 'abirritation',\n",
       " 'abirritative',\n",
       " 'abiston',\n",
       " 'abitibi',\n",
       " 'abiuret',\n",
       " 'abject',\n",
       " 'abjectedness',\n",
       " 'abjection',\n",
       " 'abjective',\n",
       " 'abjectly',\n",
       " 'abjectness',\n",
       " 'abjoint',\n",
       " 'abjudge',\n",
       " 'abjudicate',\n",
       " 'abjudication',\n",
       " 'abjunction',\n",
       " 'abjunctive',\n",
       " 'abjuration',\n",
       " 'abjuratory',\n",
       " 'abjure',\n",
       " 'abjurement',\n",
       " 'abjurer',\n",
       " 'abkar',\n",
       " 'abkari',\n",
       " 'abkhas',\n",
       " 'abkhasian',\n",
       " 'ablach',\n",
       " 'ablactate',\n",
       " 'ablactation',\n",
       " 'ablare',\n",
       " 'ablastemic',\n",
       " 'ablastous',\n",
       " 'ablate',\n",
       " 'ablation',\n",
       " 'ablatitious',\n",
       " 'ablatival',\n",
       " 'ablative',\n",
       " 'ablator',\n",
       " 'ablaut',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ableeze',\n",
       " 'ablegate',\n",
       " 'ableness',\n",
       " 'ablepharia',\n",
       " 'ablepharon',\n",
       " 'ablepharous',\n",
       " 'ablepharus',\n",
       " 'ablepsia',\n",
       " 'ableptical',\n",
       " 'ableptically',\n",
       " 'abler',\n",
       " 'ablest',\n",
       " 'ablewhackets',\n",
       " 'ablins',\n",
       " 'abloom',\n",
       " 'ablow',\n",
       " 'ablude',\n",
       " 'abluent',\n",
       " 'ablush',\n",
       " 'ablution',\n",
       " 'ablutionary',\n",
       " 'abluvion',\n",
       " 'ably',\n",
       " 'abmho',\n",
       " 'abnaki',\n",
       " 'abnegate',\n",
       " 'abnegation',\n",
       " 'abnegative',\n",
       " 'abnegator',\n",
       " 'abner',\n",
       " 'abnerval',\n",
       " 'abnet',\n",
       " 'abneural',\n",
       " 'abnormal',\n",
       " 'abnormalism',\n",
       " 'abnormalist',\n",
       " 'abnormality',\n",
       " 'abnormalize',\n",
       " 'abnormally',\n",
       " 'abnormalness',\n",
       " 'abnormity',\n",
       " 'abnormous',\n",
       " 'abnumerable',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'abobra',\n",
       " 'abode',\n",
       " 'abodement',\n",
       " 'abody',\n",
       " 'abohm',\n",
       " 'aboil',\n",
       " 'abolish',\n",
       " 'abolisher',\n",
       " 'abolishment',\n",
       " 'abolition',\n",
       " 'abolitionary',\n",
       " 'abolitionism',\n",
       " 'abolitionist',\n",
       " 'abolitionize',\n",
       " 'abolla',\n",
       " 'aboma',\n",
       " 'abomasum',\n",
       " 'abomasus',\n",
       " 'abominable',\n",
       " 'abominableness',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abominator',\n",
       " 'abomine',\n",
       " 'abongo',\n",
       " 'aboon',\n",
       " 'aborad',\n",
       " 'aboral',\n",
       " 'aborally',\n",
       " 'abord',\n",
       " 'aboriginal',\n",
       " 'aboriginality',\n",
       " 'aboriginally',\n",
       " 'aboriginary',\n",
       " 'aborigine',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborticide',\n",
       " 'abortient',\n",
       " 'abortifacient',\n",
       " 'abortin',\n",
       " 'abortion',\n",
       " 'abortional',\n",
       " 'abortionist',\n",
       " 'abortive',\n",
       " 'abortively',\n",
       " 'abortiveness',\n",
       " 'abortus',\n",
       " 'abouchement',\n",
       " 'abound',\n",
       " 'abounder',\n",
       " 'abounding',\n",
       " 'aboundingly',\n",
       " 'about',\n",
       " 'abouts',\n",
       " 'above',\n",
       " 'aboveboard',\n",
       " 'abovedeck',\n",
       " 'aboveground',\n",
       " 'aboveproof',\n",
       " 'abovestairs',\n",
       " 'abox',\n",
       " 'abracadabra',\n",
       " 'abrachia',\n",
       " 'abradant',\n",
       " 'abrade',\n",
       " 'abrader',\n",
       " 'abraham',\n",
       " 'abrahamic',\n",
       " 'abrahamidae',\n",
       " 'abrahamite',\n",
       " 'abrahamitic',\n",
       " 'abraid',\n",
       " 'abram',\n",
       " 'abramis',\n",
       " 'abranchial',\n",
       " 'abranchialism',\n",
       " 'abranchian',\n",
       " 'abranchiata',\n",
       " 'abranchiate',\n",
       " 'abranchious',\n",
       " 'abrasax',\n",
       " 'abrase',\n",
       " 'abrash',\n",
       " 'abrasiometer',\n",
       " 'abrasion',\n",
       " 'abrasive',\n",
       " 'abrastol',\n",
       " 'abraum',\n",
       " 'abraxas',\n",
       " 'abreact',\n",
       " 'abreaction',\n",
       " 'abreast',\n",
       " 'abrenounce',\n",
       " 'abret',\n",
       " 'abrico',\n",
       " 'abridge',\n",
       " 'abridgeable',\n",
       " 'abridged',\n",
       " 'abridgedly',\n",
       " 'abridger',\n",
       " 'abridgment',\n",
       " 'abrim',\n",
       " 'abrin',\n",
       " 'abristle',\n",
       " 'abroach',\n",
       " 'abroad',\n",
       " 'abrocoma',\n",
       " 'abrocome',\n",
       " 'abrogable',\n",
       " 'abrogate',\n",
       " 'abrogation',\n",
       " 'abrogative',\n",
       " 'abrogator',\n",
       " 'abroma',\n",
       " 'abronia',\n",
       " 'abrook',\n",
       " 'abrotanum',\n",
       " 'abrotine',\n",
       " 'abrupt',\n",
       " 'abruptedly',\n",
       " 'abruption',\n",
       " 'abruptly',\n",
       " 'abruptness',\n",
       " 'abrus',\n",
       " 'absalom',\n",
       " 'absampere',\n",
       " 'absaroka',\n",
       " 'absarokite',\n",
       " 'abscess',\n",
       " 'abscessed',\n",
       " 'abscession',\n",
       " 'abscessroot',\n",
       " 'abscind',\n",
       " 'abscise',\n",
       " 'abscision',\n",
       " 'absciss',\n",
       " 'abscissa',\n",
       " 'abscissae',\n",
       " 'abscisse',\n",
       " 'abscission',\n",
       " 'absconce',\n",
       " 'abscond',\n",
       " 'absconded',\n",
       " 'abscondedly',\n",
       " 'abscondence',\n",
       " 'absconder',\n",
       " 'absconsa',\n",
       " 'abscoulomb',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absentation',\n",
       " 'absentee',\n",
       " 'absenteeism',\n",
       " 'absenteeship',\n",
       " 'absenter',\n",
       " 'absently',\n",
       " 'absentment',\n",
       " 'absentmindedly',\n",
       " 'absentness',\n",
       " 'absfarad',\n",
       " 'abshenry',\n",
       " 'absi',\n",
       " 'absinthe',\n",
       " 'absinthial',\n",
       " 'absinthian',\n",
       " 'absinthiate',\n",
       " 'absinthic',\n",
       " 'absinthin',\n",
       " 'absinthine',\n",
       " 'absinthism',\n",
       " 'absinthismic',\n",
       " 'absinthium',\n",
       " 'absinthol',\n",
       " 'absit',\n",
       " 'absmho',\n",
       " 'absohm',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absoluteness',\n",
       " 'absolution',\n",
       " 'absolutism',\n",
       " 'absolutist',\n",
       " 'absolutistic',\n",
       " 'absolutistically',\n",
       " 'absolutive',\n",
       " 'absolutization',\n",
       " 'absolutize',\n",
       " 'absolutory',\n",
       " 'absolvable',\n",
       " 'absolvatory',\n",
       " 'absolve',\n",
       " 'absolvent',\n",
       " 'absolver',\n",
       " 'absolvitor',\n",
       " 'absolvitory',\n",
       " 'absonant',\n",
       " 'absonous',\n",
       " 'absorb',\n",
       " 'absorbability',\n",
       " 'absorbable',\n",
       " 'absorbed',\n",
       " 'absorbedly',\n",
       " 'absorbedness',\n",
       " 'absorbefacient',\n",
       " 'absorbency',\n",
       " 'absorbent',\n",
       " 'absorber',\n",
       " 'absorbing',\n",
       " 'absorbingly',\n",
       " 'absorbition',\n",
       " 'absorpt',\n",
       " 'absorptance',\n",
       " 'absorptiometer',\n",
       " 'absorptiometric',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'absorptively',\n",
       " 'absorptiveness',\n",
       " 'absorptivity',\n",
       " 'absquatulate',\n",
       " 'abstain',\n",
       " 'abstainer',\n",
       " 'abstainment',\n",
       " 'abstemious',\n",
       " 'abstemiously',\n",
       " 'abstemiousness',\n",
       " 'abstention',\n",
       " 'abstentionist',\n",
       " 'abstentious',\n",
       " 'absterge',\n",
       " 'abstergent',\n",
       " 'abstersion',\n",
       " 'abstersive',\n",
       " 'abstersiveness',\n",
       " 'abstinence',\n",
       " 'abstinency',\n",
       " 'abstinent',\n",
       " 'abstinential',\n",
       " 'abstinently',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'abstractedly',\n",
       " 'abstractedness',\n",
       " 'abstracter',\n",
       " 'abstraction',\n",
       " 'abstractional',\n",
       " 'abstractionism',\n",
       " 'abstractionist',\n",
       " 'abstractitious',\n",
       " 'abstractive',\n",
       " 'abstractively',\n",
       " 'abstractiveness',\n",
       " 'abstractly',\n",
       " 'abstractness',\n",
       " 'abstractor',\n",
       " 'abstrahent',\n",
       " 'abstricted',\n",
       " 'abstriction',\n",
       " 'abstruse',\n",
       " 'abstrusely',\n",
       " 'abstruseness',\n",
       " 'abstrusion',\n",
       " 'abstrusity',\n",
       " 'absume',\n",
       " 'absumption',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'absvolt',\n",
       " 'absyrtus',\n",
       " 'abterminal',\n",
       " 'abthain',\n",
       " 'abthainrie',\n",
       " 'abthainry',\n",
       " 'abthanage',\n",
       " 'abu',\n",
       " 'abu',\n",
       " 'abucco',\n",
       " 'abulia',\n",
       " 'abulic',\n",
       " 'abulomania',\n",
       " 'abuna',\n",
       " 'abundance',\n",
       " 'abundancy',\n",
       " 'abundant',\n",
       " 'abundantia',\n",
       " 'abundantly',\n",
       " 'abura',\n",
       " 'aburabozu',\n",
       " 'aburban',\n",
       " 'aburst',\n",
       " 'aburton',\n",
       " 'abusable',\n",
       " 'abuse',\n",
       " 'abusedly',\n",
       " 'abusee',\n",
       " 'abuseful',\n",
       " 'abusefully',\n",
       " 'abusefulness',\n",
       " 'abuser',\n",
       " 'abusion',\n",
       " 'abusious',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abusiveness',\n",
       " 'abut',\n",
       " 'abuta',\n",
       " 'abutilon',\n",
       " 'abutment',\n",
       " 'abuttal',\n",
       " 'abutter',\n",
       " 'abutting',\n",
       " 'abuzz',\n",
       " 'abvolt',\n",
       " 'abwab',\n",
       " 'aby',\n",
       " 'abysm',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'abyssal',\n",
       " 'abyssinian',\n",
       " 'abyssobenthonic',\n",
       " 'abyssolith',\n",
       " 'abyssopelagic',\n",
       " 'acacatechin',\n",
       " 'acacatechol',\n",
       " 'acacetin',\n",
       " 'acacia',\n",
       " 'acacian',\n",
       " 'acaciin',\n",
       " 'acacin',\n",
       " 'academe',\n",
       " 'academial',\n",
       " 'academian',\n",
       " 'academic',\n",
       " 'academic',\n",
       " 'academical',\n",
       " 'academically',\n",
       " 'academicals',\n",
       " 'academician',\n",
       " 'academicism',\n",
       " 'academism',\n",
       " 'academist',\n",
       " 'academite',\n",
       " 'academization',\n",
       " 'academize',\n",
       " 'academus',\n",
       " 'academy',\n",
       " 'acadia',\n",
       " 'acadialite',\n",
       " 'acadian',\n",
       " 'acadie',\n",
       " 'acaena',\n",
       " 'acajou',\n",
       " 'acaleph',\n",
       " 'acalepha',\n",
       " 'acalephae',\n",
       " 'acalephan',\n",
       " 'acalephoid',\n",
       " 'acalycal',\n",
       " 'acalycine',\n",
       " 'acalycinous',\n",
       " 'acalyculate',\n",
       " 'acalypha',\n",
       " 'acalypterae',\n",
       " 'acalyptrata',\n",
       " 'acalyptratae',\n",
       " 'acalyptrate',\n",
       " 'acamar',\n",
       " 'acampsia',\n",
       " 'acana',\n",
       " 'acanaceous',\n",
       " 'acanonical',\n",
       " 'acanth',\n",
       " 'acantha',\n",
       " 'acanthaceae',\n",
       " 'acanthaceous',\n",
       " 'acanthad',\n",
       " 'acantharia',\n",
       " 'acanthia',\n",
       " 'acanthial',\n",
       " 'acanthin',\n",
       " 'acanthine',\n",
       " 'acanthion',\n",
       " 'acanthite',\n",
       " 'acanthocarpous',\n",
       " 'acanthocephala',\n",
       " 'acanthocephalan',\n",
       " 'acanthocephali',\n",
       " 'acanthocephalous',\n",
       " 'acanthocereus',\n",
       " 'acanthocladous',\n",
       " 'acanthodea',\n",
       " 'acanthodean',\n",
       " 'acanthodei',\n",
       " 'acanthodes',\n",
       " 'acanthodian',\n",
       " 'acanthodidae',\n",
       " 'acanthodii',\n",
       " 'acanthodini',\n",
       " 'acanthoid',\n",
       " 'acantholimon',\n",
       " 'acanthological',\n",
       " 'acanthology',\n",
       " 'acantholysis',\n",
       " 'acanthoma',\n",
       " 'acanthomeridae',\n",
       " 'acanthon',\n",
       " 'acanthopanax',\n",
       " 'acanthophis',\n",
       " 'acanthophorous',\n",
       " 'acanthopod',\n",
       " 'acanthopodous',\n",
       " 'acanthopomatous',\n",
       " 'acanthopore',\n",
       " 'acanthopteran',\n",
       " 'acanthopteri',\n",
       " 'acanthopterous',\n",
       " 'acanthopterygian',\n",
       " 'acanthopterygii',\n",
       " 'acanthosis',\n",
       " 'acanthous',\n",
       " 'acanthuridae',\n",
       " 'acanthurus',\n",
       " 'acanthus',\n",
       " 'acapnia',\n",
       " 'acapnial',\n",
       " 'acapsular',\n",
       " 'acapu',\n",
       " 'acapulco',\n",
       " 'acara',\n",
       " 'acarapis',\n",
       " 'acardia',\n",
       " 'acardiac',\n",
       " 'acari',\n",
       " 'acarian',\n",
       " 'acariasis',\n",
       " 'acaricidal',\n",
       " 'acaricide',\n",
       " 'acarid',\n",
       " 'acarida',\n",
       " 'acaridea',\n",
       " 'acaridean',\n",
       " 'acaridomatium',\n",
       " 'acariform',\n",
       " 'acarina',\n",
       " 'acarine',\n",
       " 'acarinosis',\n",
       " 'acarocecidium',\n",
       " 'acarodermatitis',\n",
       " 'acaroid',\n",
       " 'acarol',\n",
       " 'acarologist',\n",
       " 'acarology',\n",
       " 'acarophilous',\n",
       " 'acarophobia',\n",
       " 'acarotoxic',\n",
       " 'acarpelous',\n",
       " 'acarpous',\n",
       " 'acarus',\n",
       " 'acastus',\n",
       " 'acatalectic',\n",
       " 'acatalepsia',\n",
       " 'acatalepsy',\n",
       " 'acataleptic',\n",
       " 'acatallactic',\n",
       " 'acatamathesia',\n",
       " 'acataphasia',\n",
       " 'acataposis',\n",
       " 'acatastasia',\n",
       " 'acatastatic',\n",
       " 'acate',\n",
       " 'acategorical',\n",
       " 'acatery',\n",
       " 'acatharsia',\n",
       " 'acatharsy',\n",
       " 'acatholic',\n",
       " 'acaudal',\n",
       " 'acaudate',\n",
       " 'acaulescent',\n",
       " 'acauline',\n",
       " 'acaulose',\n",
       " 'acaulous',\n",
       " 'acca',\n",
       " 'accede',\n",
       " 'accedence',\n",
       " 'acceder',\n",
       " 'accelerable',\n",
       " 'accelerando',\n",
       " 'accelerant',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'acceleratedly',\n",
       " 'acceleration',\n",
       " 'accelerative',\n",
       " 'accelerator',\n",
       " 'acceleratory',\n",
       " 'accelerograph',\n",
       " 'accelerometer',\n",
       " 'accend',\n",
       " 'accendibility',\n",
       " 'accendible',\n",
       " 'accension',\n",
       " 'accensor',\n",
       " 'accent',\n",
       " 'accentless',\n",
       " 'accentor',\n",
       " 'accentuable',\n",
       " 'accentual',\n",
       " 'accentuality',\n",
       " 'accentually',\n",
       " 'accentuate',\n",
       " 'accentuation',\n",
       " 'accentuator',\n",
       " 'accentus',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptableness',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'acceptancy',\n",
       " 'acceptant',\n",
       " 'acceptation',\n",
       " 'accepted',\n",
       " 'acceptedly',\n",
       " 'accepter',\n",
       " 'acceptilate',\n",
       " 'acceptilation',\n",
       " 'acception',\n",
       " 'acceptive',\n",
       " 'acceptor',\n",
       " 'acceptress',\n",
       " 'accerse',\n",
       " 'accersition',\n",
       " 'accersitor',\n",
       " 'access',\n",
       " 'accessarily',\n",
       " 'accessariness',\n",
       " 'accessary',\n",
       " 'accessaryship',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessibly',\n",
       " 'accession',\n",
       " 'accessional',\n",
       " 'accessioner',\n",
       " 'accessive',\n",
       " 'accessively',\n",
       " 'accessless',\n",
       " 'accessorial',\n",
       " 'accessorily',\n",
       " 'accessoriness',\n",
       " 'accessorius',\n",
       " 'accessory',\n",
       " 'accidence',\n",
       " 'accidency',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentalism',\n",
       " 'accidentalist',\n",
       " 'accidentality',\n",
       " 'accidentally',\n",
       " 'accidentalness',\n",
       " 'accidented',\n",
       " 'accidential',\n",
       " 'accidentiality',\n",
       " 'accidently',\n",
       " 'accidia',\n",
       " 'accidie',\n",
       " 'accinge',\n",
       " 'accipient',\n",
       " 'accipiter',\n",
       " 'accipitral',\n",
       " 'accipitrary',\n",
       " 'accipitres',\n",
       " 'accipitrine',\n",
       " 'accismus',\n",
       " 'accite',\n",
       " 'acclaim',\n",
       " 'acclaimable',\n",
       " 'acclaimer',\n",
       " 'acclamation',\n",
       " 'acclamator',\n",
       " 'acclamatory',\n",
       " 'acclimatable',\n",
       " 'acclimatation',\n",
       " 'acclimate',\n",
       " 'acclimatement',\n",
       " 'acclimation',\n",
       " 'acclimatizable',\n",
       " 'acclimatization',\n",
       " 'acclimatize',\n",
       " 'acclimatizer',\n",
       " 'acclimature',\n",
       " 'acclinal',\n",
       " 'acclinate',\n",
       " 'acclivitous',\n",
       " 'acclivity',\n",
       " 'acclivous',\n",
       " 'accloy',\n",
       " 'accoast',\n",
       " 'accoil',\n",
       " 'accolade',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accolent',\n",
       " 'accolle',\n",
       " 'accombination',\n",
       " 'accommodable',\n",
       " 'accommodableness',\n",
       " 'accommodate',\n",
       " 'accommodately',\n",
       " 'accommodateness',\n",
       " 'accommodating',\n",
       " 'accommodatingly',\n",
       " 'accommodation',\n",
       " 'accommodational',\n",
       " 'accommodative',\n",
       " 'accommodativeness',\n",
       " 'accommodator',\n",
       " 'accompanier',\n",
       " 'accompaniment',\n",
       " 'accompanimental',\n",
       " 'accompanist',\n",
       " 'accompany',\n",
       " 'accompanyist',\n",
       " 'accompletive',\n",
       " 'accomplice',\n",
       " 'accompliceship',\n",
       " 'accomplicity',\n",
       " 'accomplish',\n",
       " 'accomplishable',\n",
       " 'accomplished',\n",
       " 'accomplisher',\n",
       " 'accomplishment',\n",
       " 'accomplisht',\n",
       " 'accompt',\n",
       " 'accord',\n",
       " 'accordable',\n",
       " 'accordance',\n",
       " 'accordancy',\n",
       " 'accordant',\n",
       " ...]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_words = [i.lower() for i in words.words()]\n",
    "eng_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a8675c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:31.099983Z",
     "start_time": "2022-06-30T16:05:31.046222Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234377"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(eng_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3bc7455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:36.044953Z",
     "start_time": "2022-06-30T16:05:31.102937Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43102        coriparian\n",
       "112007            mason\n",
       "183961          snipnos\n",
       "56442         doctoress\n",
       "155695          primula\n",
       "167965         resplend\n",
       "82035         hamirostr\n",
       "29988          capitoul\n",
       "170809          roomthi\n",
       "86799             homer\n",
       "43001            cordon\n",
       "46130        crushingli\n",
       "39945          commeddl\n",
       "159326       psychosexu\n",
       "81521           hackman\n",
       "131471      opisthodomu\n",
       "58063           drugman\n",
       "101767            kevin\n",
       "110079         magnetod\n",
       "119298          moonris\n",
       "35706      chromatophil\n",
       "78925            gopher\n",
       "42170        contradebt\n",
       "76919          gesneria\n",
       "211777    unbeseemingli\n",
       "37980     clypeastridea\n",
       "174736          schemat\n",
       "94879               ink\n",
       "179259          seventh\n",
       "211589           unbapt\n",
       "dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "eng_words_stem = [ps.stem(i) for i in eng_words]\n",
    "pd.Series(eng_words_stem).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "610242ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:36.091969Z",
     "start_time": "2022-06-30T16:05:36.047600Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178311"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(eng_words_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab12e9a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:36.957424Z",
     "start_time": "2022-06-30T16:05:36.094310Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235216               ziamet\n",
       "71941               footway\n",
       "157428            proseucha\n",
       "198911             taxiauto\n",
       "229528        voicelessness\n",
       "120803        munchausenize\n",
       "59397           ecospecific\n",
       "235366              zoarial\n",
       "59102             easternly\n",
       "17818                balaam\n",
       "141882           pentaquine\n",
       "17560                badaga\n",
       "34200                chelys\n",
       "140092          passivation\n",
       "189697           sticktight\n",
       "175996           scrobicule\n",
       "101841                khaya\n",
       "34958             chloremia\n",
       "217874                 unie\n",
       "57278             doughfoot\n",
       "9208                 ansate\n",
       "127523    nonsubstantiation\n",
       "66982               explode\n",
       "7356            amygdalitis\n",
       "59737              eelspear\n",
       "34465           chickenhood\n",
       "19665             beaverish\n",
       "146966                pined\n",
       "223521          unstumbling\n",
       "224499           untyrannic\n",
       "dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "eng_words_lem = [lm.lemmatize(i) for i in eng_words]\n",
    "pd.Series(eng_words_lem).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f189495d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:36.964965Z",
     "start_time": "2022-06-30T16:05:36.960417Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_words_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35dae45",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.6 4th cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5a27afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:05:37.003298Z",
     "start_time": "2022-06-30T16:05:36.967816Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_text_4(doc, \n",
    "                   rejoin=False, \n",
    "                   lemm_or_stemm=\"stem\",\n",
    "                   list_rare_words=None, \n",
    "                   min_len_word=3,\n",
    "                   force_is_alpha=True, \n",
    "                   eng_words=None) : \n",
    "    \"\"\"cf process_text_3 but with selection of only english words\n",
    "    \n",
    "    positional arguments : \n",
    "    -----------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "    \n",
    "    opt args : \n",
    "    -----------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    lemm_or_stemm : str : if lem do lemmentize else stemmentize  \n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : the minimum length of words to not exclude\n",
    "    force_is_alpha : int : if 1, exclude all tokens with a numeric character\n",
    "    eng_words : list : list of english words\n",
    "    \n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # classics stopwords\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # no more len words\n",
    "    more_than_N =  [w for w in non_rare_tokens if len(w) >= min_len_word  ]\n",
    "    \n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else :\n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_tokens ]\n",
    "    else : \n",
    "        trans = PorterStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens ]\n",
    "\n",
    "    ###########################################################\n",
    "    ###########################################################\n",
    "        \n",
    "    # in english \n",
    "    if eng_words :\n",
    "        engl_text = [i for i in trans_text if i in eng_words]\n",
    "    else :\n",
    "        engl_text = trans_text\n",
    "    \n",
    "    ###########################################################\n",
    "    ###########################################################\n",
    "        \n",
    "    #  return a list or a string\n",
    "    if rejoin : \n",
    "        return \" \".join(engl_text)\n",
    "    \n",
    "    return engl_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a5660a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:53.546475Z",
     "start_time": "2022-06-30T16:05:37.006256Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deed',\n",
       " 'reason',\n",
       " 'earthquak',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgiv',\n",
       " 'fire',\n",
       " 'near',\n",
       " 'resid',\n",
       " 'ask',\n",
       " 'shelter',\n",
       " 'place',\n",
       " 'offic',\n",
       " 'evacu',\n",
       " 'shelter',\n",
       " 'place',\n",
       " 'order',\n",
       " 'peopl',\n",
       " 'receiv',\n",
       " 'wildfir',\n",
       " 'evacu',\n",
       " 'order',\n",
       " 'california',\n",
       " 'got',\n",
       " 'sent',\n",
       " 'photo',\n",
       " 'alaska',\n",
       " 'smoke',\n",
       " 'wildfir',\n",
       " 'school']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_text_4(raw_corpus, rejoin=False, list_rare_words=list_unique_words, eng_words=eng_words_stem)\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9181a359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:53.558077Z",
     "start_time": "2022-06-30T16:08:53.549705Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3461"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82d12c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:53.567372Z",
     "start_time": "2022-06-30T16:08:53.559893Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7503"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55afb159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:53.574686Z",
     "start_time": "2022-06-30T16:08:53.569962Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dub',\n",
       " 'mxhrextrkh',\n",
       " 'ctijdpxabkdogs',\n",
       " 'splatling',\n",
       " 'foothill',\n",
       " 'designsso',\n",
       " 'thatrussianman',\n",
       " 'waterboarding',\n",
       " 'writingtips',\n",
       " 'salmanmydarling',\n",
       " 'ps3',\n",
       " 'xboxhttps',\n",
       " 'qr1l2jyuez',\n",
       " 'nester',\n",
       " 'switching',\n",
       " 'dipping',\n",
       " 'pantherattackthere',\n",
       " 'dieanpink95',\n",
       " 'limitsabe',\n",
       " 'yu_nita99',\n",
       " 'sivan',\n",
       " 'pantherattacki',\n",
       " 'camilla_33',\n",
       " 'uooygbb6az',\n",
       " 'akq4rwjfvlcheck',\n",
       " 'skippy6gaming',\n",
       " 'slttorrlhswho',\n",
       " 'craykain',\n",
       " 'lavalet',\n",
       " 'basalt']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d0f226e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:53.580721Z",
     "start_time": "2022-06-30T16:08:53.576754Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16230"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a4f9a61a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:53.588012Z",
     "start_time": "2022-06-30T16:08:53.583089Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "motorcycle     5\n",
       "blind          5\n",
       "ices           5\n",
       "remain         5\n",
       "md             5\n",
       "mental         5\n",
       "loves          5\n",
       "depth          5\n",
       "extra          5\n",
       "leaves         5\n",
       "subs           5\n",
       "judge          5\n",
       "earners        5\n",
       "operations     5\n",
       "reduced        5\n",
       "catch          5\n",
       "stephen        5\n",
       "quest          5\n",
       "reviews        5\n",
       "responsible    5\n",
       "motor          5\n",
       "flying         5\n",
       "smithsonian    5\n",
       "52             5\n",
       "34             5\n",
       "losses         5\n",
       "desires        5\n",
       "pulls          5\n",
       "mood           5\n",
       "tubestrike     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_min_5_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ccce498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:53.594405Z",
     "start_time": "2022-06-30T16:08:53.589873Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20275"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0179c6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.743164Z",
     "start_time": "2022-06-30T16:08:53.597043Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_text_4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrejoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_rare_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_min_5_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meng_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meng_words_stem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m corpus[:\u001b[38;5;241m30\u001b[39m]\n",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36mprocess_text_4\u001b[0;34m(doc, rejoin, lemm_or_stemm, list_rare_words, min_len_word, force_is_alpha, eng_words)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"cf process_text_3 but with selection of only english words\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mpositional arguments : \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03ma string (if rejoin is True) or a list of tokens\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# list_unique_words\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m list_rare_words: \n\u001b[1;32m     31\u001b[0m     list_rare_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# lower\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ocr-donnees-textuelles/env/lib/python3.8/site-packages/pandas/core/generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1530\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "corpus = process_text_4(raw_corpus, rejoin=False, list_rare_words=list_min_5_words, eng_words=eng_words_stem)\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.746292Z",
     "start_time": "2022-06-30T16:08:54.746265Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ee4bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.748519Z",
     "start_time": "2022-06-30T16:08:54.748491Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623efe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.751330Z",
     "start_time": "2022-06-30T16:08:54.751300Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sns.barplot(tmp.index, tmp.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed6440",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.7 Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4abba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.753989Z",
     "start_time": "2022-06-30T16:08:54.753959Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white', \n",
    "                      stopwords=[], \n",
    "                      max_words=50).generate(\" \".join(corpus))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcda990",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. Divide the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841778c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.1 Separate 0 / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12526deb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.756670Z",
     "start_time": "2022-06-30T16:08:54.756650Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_1 = df[df.target == 1]\n",
    "df_0 = df[df.target == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fc9b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.758738Z",
     "start_time": "2022-06-30T16:08:54.758713Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_1 = \" \".join(df_1.text)\n",
    "corpus_0 = \" \".join(df_0.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec8b8e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.2 Process boths of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d404ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.761328Z",
     "start_time": "2022-06-30T16:08:54.761291Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_1 = process_text_4(corpus_1, \n",
    "                          rejoin=False, \n",
    "                          list_rare_words=list_min_5_words, \n",
    "                          eng_words=eng_words_stem)\n",
    "\n",
    "corpus_0 = process_text_4(corpus_0, \n",
    "                          rejoin=False, \n",
    "                          list_rare_words=list_min_5_words, \n",
    "                          eng_words=eng_words_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa0c90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.763728Z",
     "start_time": "2022-06-30T16:08:54.763710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white', \n",
    "                      stopwords=[], \n",
    "                      max_words = 50).generate(\" \".join(corpus_1))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e774a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.765988Z",
     "start_time": "2022-06-30T16:08:54.765947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white', \n",
    "                      stopwords=[], max_words=50).generate(\" \".join(corpus_0))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dcbe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.768787Z",
     "start_time": "2022-06-30T16:08:54.768761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(corpus_1).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56913590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.771707Z",
     "start_time": "2022-06-30T16:08:54.771687Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(corpus_0).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf262359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.773148Z",
     "start_time": "2022-06-30T16:08:54.773131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[i for i in pd.Series(corpus_1).value_counts().head(10).index \n",
    "     if i in pd.Series(corpus_0).value_counts().head(10).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e9139",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.3 5th cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e9685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.774562Z",
     "start_time": "2022-06-30T16:08:54.774545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_text_5(doc, \n",
    "                   rejoin=True, \n",
    "                   lemm_or_stemm = \"stem\", \n",
    "                   list_rare_words=None, \n",
    "                   min_len_word=3, \n",
    "                   eng_words=None) : \n",
    "    \"\"\"df v4 but exclude amp\"\"\"\n",
    " \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower and strip\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # remove stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    # drop rare tokens\n",
    "    non_rare_tokens_list = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # keep only len word > N\n",
    "    more_than_N =  [w for w in non_rare_tokens_list if len(w) >= 3 ]\n",
    "    \n",
    "    # keep only alpha not num\n",
    "    alpha_num = [w for w in more_than_N if w.isalpha()]\n",
    "    \n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_num ]\n",
    "    else : \n",
    "        trans = PorterStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_num ]\n",
    "        \n",
    "    # in english \n",
    "    if eng_words :\n",
    "        engl_text = [i for i in trans_text if i in eng_words]\n",
    "    else :\n",
    "        engl_text = trans_text\n",
    "        \n",
    "    ##########################################\n",
    "    ##########################################\n",
    "    \n",
    "    # amp\n",
    "    engl_text = [i for i in engl_text if i!=\"amp\"]\n",
    "    \n",
    "    ##########################################\n",
    "    ##########################################\n",
    "    \n",
    "    #  return a list or a string\n",
    "    if rejoin : \n",
    "        return \" \".join(engl_text)\n",
    "    \n",
    "    return engl_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afecb6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc0a6cd9",
   "metadata": {},
   "source": [
    "# 5. Final clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b24fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.776106Z",
     "start_time": "2022-06-30T16:08:54.776088Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_clean(doc) : \n",
    "    \n",
    "    new_doc = process_text_5(doc,rejoin=True, \n",
    "                             stem_or_lem=\"stem\", \n",
    "                             list_rare_words=list_min_5_words, \n",
    "                             eng_words=eng_words_lem)\n",
    "    return  new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0865a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.778066Z",
     "start_time": "2022-06-30T16:08:54.778049Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df.text.apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.781854Z",
     "start_time": "2022-06-30T16:08:54.781827Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf82f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T16:08:54.783867Z",
     "start_time": "2022-06-30T16:08:54.783848Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"data/cleaned/final_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
