{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d846cb",
   "metadata": {},
   "source": [
    "# 1. Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03838406",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1.1 Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79995afa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* We will analyze a very well known NLP dataset: tweets from disaster\n",
    "\n",
    "\n",
    "* It is a Kaggle competition, which offers a simple but good level textual dataset to be able to make its weapons in NLP\n",
    "\n",
    "\n",
    "* The dataset is here [https://www.kaggle.com/competitions/nlp-getting-started/data]\n",
    "\n",
    "\n",
    "* Please use the **train** dataset\n",
    "\n",
    "\n",
    "* In this 1st part we are going to clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562f101",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1.2 Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885639a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:47:48.424652Z",
     "start_time": "2022-06-28T20:47:48.419964Z"
    },
    "hidden": true
   },
   "source": [
    "You have to install  : \n",
    "\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* seaborn\n",
    "\n",
    "\n",
    "* nltk\n",
    "* wordcloud\n",
    "* pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97396f16",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1.3 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d56dbd5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:36.642923Z",
     "start_time": "2022-06-28T21:02:34.384247Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, sys, time, random\n",
    "\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# import spacy\n",
    "\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# import plotly as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a272cd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1.4 Downloads and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f872b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.094662Z",
     "start_time": "2022-06-28T21:02:36.647570Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/alex/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecb0d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.104388Z",
     "start_time": "2022-06-28T21:02:37.099383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765b4148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.112517Z",
     "start_time": "2022-06-28T21:02:37.108420Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init pandarallel\n",
    "\n",
    "# pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880a917",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1.5 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c046fdbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.124212Z",
     "start_time": "2022-06-28T21:02:37.117474Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['min_10_words.csv',\n",
       " 'df.csv',\n",
       " 'final_df.csv',\n",
       " 'df_cleaned.csv',\n",
       " 'unique_words.csv',\n",
       " 'finad_df.csv',\n",
       " 'min_5_words.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our file\n",
    "\n",
    "data = \"./data/cleaned/\"\n",
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e699e715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.168244Z",
     "start_time": "2022-06-28T21:02:37.127662Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataframe\n",
    "\n",
    "fn = data + 'df_cleaned.csv'\n",
    "df = pd.read_csv(fn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab788d1d",
   "metadata": {},
   "source": [
    "# 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea19425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.180747Z",
     "start_time": "2022-06-28T21:02:37.172584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@kirkmin after listening to you demolish @BartHubbuch on @weei I can't wait to bait my patriot hater co-workers into a Brady discussion\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a random document\n",
    "\n",
    "doc = df.text.sample(1)\n",
    "doc = doc.values[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4add6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1754801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:04:10.622579Z",
     "start_time": "2022-06-28T21:04:10.617271Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@kirkmin after listening to you demolish @barthubbuch on @weei i can't wait to bait my patriot hater co-workers into a brady discussion\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower\n",
    "\n",
    "doc = doc.lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423cf19",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a5c20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.212222Z",
     "start_time": "2022-06-28T21:02:37.195613Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'kirkmin',\n",
       " 'after',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'you',\n",
       " 'demolish',\n",
       " '@',\n",
       " 'barthubbuch',\n",
       " 'on',\n",
       " '@',\n",
       " 'weei',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'wait',\n",
       " 'to',\n",
       " 'bait',\n",
       " 'my',\n",
       " 'patriot',\n",
       " 'hater',\n",
       " 'co-workers',\n",
       " 'into',\n",
       " 'a',\n",
       " 'brady',\n",
       " 'discussion']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "\n",
    "tokens = word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f7ee9a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.223424Z",
     "start_time": "2022-06-28T21:02:37.217641Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f6a396b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.239798Z",
     "start_time": "2022-06-28T21:02:37.228436Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'kirkmin',\n",
       " 'after',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'you',\n",
       " 'demolish',\n",
       " '@',\n",
       " 'barthubbuch',\n",
       " 'on',\n",
       " '@',\n",
       " 'weei',\n",
       " 'i',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'wait',\n",
       " 'to',\n",
       " 'bait',\n",
       " 'my',\n",
       " 'patriot',\n",
       " 'hater',\n",
       " 'co',\n",
       " '-',\n",
       " 'workers',\n",
       " 'into',\n",
       " 'a',\n",
       " 'brady',\n",
       " 'discussion']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an other tokenize\n",
    "\n",
    "tokens = wordpunct_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc81a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.250416Z",
     "start_time": "2022-06-28T21:02:37.243930Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59405346",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a6ec4db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.262237Z",
     "start_time": "2022-06-28T21:02:37.254110Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'off', \"should've\", 'shan', 'i', 'myself', \"you'll\", 'o', 'my', \"couldn't\", 'which', 'me', \"didn't\", 'wasn', \"wasn't\", 'these', 'doing', 'this', 'below', 'her', 'be', 'too', \"weren't\", 'your', 'they', 'that', 'most', 'with', 'who', 'had', 'she', 'the', 'd', 'doesn', 'our', 'being', 'further', 'we', 'other', 'shouldn', 'having', 'hasn', 'through', 'all', 'what', 'their', 'those', 'of', 'each', 'wouldn', 'but', 'did', \"mustn't\", \"shan't\", 'down', 'have', \"it's\", \"doesn't\", 'before', 'his', 'very', 'few', 'any', 'on', 'hers', \"aren't\", 'own', 'aren', 'can', 'whom', 'nor', \"don't\", 'itself', 'hadn', 'ma', 'ours', 'here', \"shouldn't\", 'after', 'am', 'was', 'if', 'between', \"won't\", 'an', 'both', 'couldn', 'while', 'for', 'it', 'will', 'only', 'again', 'him', 'how', 'isn', 'against', 'about', 'to', \"isn't\", 'is', 'and', \"that'll\", 'are', 'than', 'needn', \"needn't\", \"she's\", 'so', 'same', 'themselves', 'now', 'll', 'didn', 'there', 'from', 'don', 'you', 'where', 'out', \"you're\", 'a', 'does', 'theirs', 'above', 'at', 'once', \"you'd\", \"hasn't\", 'mustn', 'has', \"wouldn't\", 'up', 'yourself', \"hadn't\", 'then', 're', 'why', 'weren', 't', 'such', 've', 'he', 'during', 'not', 'himself', 'its', 'or', 's', 'mightn', 'won', 'yourselves', 'because', 'in', 'ain', 'over', 'when', \"haven't\", 'no', 'into', 'herself', 'under', 'as', 'by', 'some', 'them', 'y', 'm', 'do', 'until', 'yours', 'were', 'more', 'been', 'haven', 'should', \"you've\", 'ourselves', \"mightn't\", 'just'}\n"
     ]
    }
   ],
   "source": [
    "# stop_words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4792ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.274878Z",
     "start_time": "2022-06-28T21:02:37.266783Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'kirkmin',\n",
       " 'listening',\n",
       " 'demolish',\n",
       " '@',\n",
       " 'barthubbuch',\n",
       " '@',\n",
       " 'weei',\n",
       " \"'\",\n",
       " 'wait',\n",
       " 'bait',\n",
       " 'patriot',\n",
       " 'hater',\n",
       " 'co',\n",
       " '-',\n",
       " 'workers',\n",
       " 'brady',\n",
       " 'discussion']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24d4fd84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:37.285924Z",
     "start_time": "2022-06-28T21:02:37.278922Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eda51a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:39.188444Z",
     "start_time": "2022-06-28T21:02:39.175736Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kirkmin',\n",
       " 'after',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'you',\n",
       " 'demolish',\n",
       " 'barthubbuch',\n",
       " 'on',\n",
       " 'weei',\n",
       " 'i',\n",
       " 'can',\n",
       " 't',\n",
       " 'wait',\n",
       " 'to',\n",
       " 'bait',\n",
       " 'my',\n",
       " 'patriot',\n",
       " 'hater',\n",
       " 'co',\n",
       " 'workers',\n",
       " 'into',\n",
       " 'a',\n",
       " 'brady',\n",
       " 'discussion']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an other tokensizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa958d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:39.301605Z",
     "start_time": "2022-06-28T21:02:39.295256Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba57384b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:39.466524Z",
     "start_time": "2022-06-28T21:02:39.460359Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kirkmin',\n",
       " 'listening',\n",
       " 'demolish',\n",
       " 'barthubbuch',\n",
       " 'weei',\n",
       " 'wait',\n",
       " 'bait',\n",
       " 'patriot',\n",
       " 'hater',\n",
       " 'co',\n",
       " 'workers',\n",
       " 'brady',\n",
       " 'discussion']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27acc154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:39.627850Z",
     "start_time": "2022-06-28T21:02:39.622441Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1148b9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.4 First cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fbd3174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:39.783079Z",
     "start_time": "2022-06-28T21:02:39.776695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_text_1(doc, rejoin=False) : \n",
    "    \"\"\"basic function of text processing \"\"\"\n",
    "    \n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    if rejoin : \n",
    "        return \" \".join(cleaned_tokens_list)\n",
    "    \n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eafd633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:39.934103Z",
     "start_time": "2022-06-28T21:02:39.926944Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kirkmin',\n",
       " 'listening',\n",
       " 'demolish',\n",
       " 'barthubbuch',\n",
       " 'weei',\n",
       " 'wait',\n",
       " 'bait',\n",
       " 'patriot',\n",
       " 'hater',\n",
       " 'co',\n",
       " 'workers',\n",
       " 'brady',\n",
       " 'discussion']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text_1(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdea23",
   "metadata": {},
   "source": [
    "# 3. Working on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66489ad6",
   "metadata": {},
   "source": [
    "## 3.1 Build raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "047ce4ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:40.460214Z",
     "start_time": "2022-06-28T21:02:40.448598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us allForest fire near La Ronge Sask. CanadaAll residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected13,000 people receive #wildfires evacuation orders in California Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areasI'm on top of the hill and I can see a fire in the woods...There's an emergency evacuation happening now in the building across the streetI'm afraid that the tornado is coming to our area...Three people died from the heat wave so farHaha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding#raining #flooding #Florida #TampaBay #T\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join all corpus\n",
    "\n",
    "raw_corpus = \"\".join(df.text.values)\n",
    "raw_corpus[:1_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8af82dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:41.237782Z",
     "start_time": "2022-06-28T21:02:41.151057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deeds',\n",
       " 'reason',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'us',\n",
       " 'allforest',\n",
       " 'fire',\n",
       " 'near',\n",
       " 'la',\n",
       " 'ronge',\n",
       " 'sask',\n",
       " 'canadaall',\n",
       " 'residents',\n",
       " 'asked',\n",
       " 'shelter',\n",
       " 'place',\n",
       " 'notified',\n",
       " 'officers',\n",
       " 'evacuation',\n",
       " 'shelter',\n",
       " 'place',\n",
       " 'orders',\n",
       " 'expected13',\n",
       " '000',\n",
       " 'people',\n",
       " 'receive',\n",
       " 'wildfires',\n",
       " 'evacuation',\n",
       " 'orders',\n",
       " 'california',\n",
       " 'got',\n",
       " 'sent',\n",
       " 'photo',\n",
       " 'ruby',\n",
       " 'alaska',\n",
       " 'smoke',\n",
       " 'wildfires',\n",
       " 'pours',\n",
       " 'school',\n",
       " 'rockyfire',\n",
       " 'update',\n",
       " 'california',\n",
       " 'hwy',\n",
       " '20',\n",
       " 'closed',\n",
       " 'directions',\n",
       " 'due',\n",
       " 'lake',\n",
       " 'county',\n",
       " 'fire',\n",
       " 'cafire',\n",
       " 'wildfires',\n",
       " 'flood',\n",
       " 'disaster',\n",
       " 'heavy',\n",
       " 'rain',\n",
       " 'causes',\n",
       " 'flash',\n",
       " 'flooding',\n",
       " 'streets',\n",
       " 'manitou',\n",
       " 'colorado',\n",
       " 'springs',\n",
       " 'areasi',\n",
       " 'top',\n",
       " 'hill',\n",
       " 'see',\n",
       " 'fire',\n",
       " 'woods',\n",
       " 'emergency',\n",
       " 'evacuation',\n",
       " 'happening',\n",
       " 'building',\n",
       " 'across',\n",
       " 'streeti',\n",
       " 'afraid',\n",
       " 'tornado',\n",
       " 'coming',\n",
       " 'area',\n",
       " 'three',\n",
       " 'people',\n",
       " 'died',\n",
       " 'heat',\n",
       " 'wave',\n",
       " 'farhaha',\n",
       " 'south',\n",
       " 'tampa',\n",
       " 'getting',\n",
       " 'flooded',\n",
       " 'hah',\n",
       " 'wait',\n",
       " 'second',\n",
       " 'live',\n",
       " 'south',\n",
       " 'tampa',\n",
       " 'gonna',\n",
       " 'gonna',\n",
       " 'fvck',\n",
       " 'flooding',\n",
       " 'raining',\n",
       " 'flooding',\n",
       " 'florida',\n",
       " 'tampabay',\n",
       " 'tampa',\n",
       " '18',\n",
       " '19',\n",
       " 'days',\n",
       " 'lost',\n",
       " 'count',\n",
       " 'flood',\n",
       " 'bago',\n",
       " 'myanmar',\n",
       " 'arrived',\n",
       " 'bagodamage',\n",
       " 'school',\n",
       " 'bus',\n",
       " '80',\n",
       " 'multi',\n",
       " 'car',\n",
       " 'crash',\n",
       " 'breaking',\n",
       " 'man',\n",
       " 'love',\n",
       " 'fruitssummer',\n",
       " 'lovelymy',\n",
       " 'car',\n",
       " 'fastwhat',\n",
       " 'goooooooaaaaaal',\n",
       " 'ridiculous',\n",
       " 'london',\n",
       " 'cool',\n",
       " 'love',\n",
       " 'skiingwhat',\n",
       " 'wonderful',\n",
       " 'day',\n",
       " 'loooooolno',\n",
       " 'way',\n",
       " 'eat',\n",
       " 'shitwas',\n",
       " 'nyc',\n",
       " 'last',\n",
       " 'week',\n",
       " 'love',\n",
       " 'girlfriendcooool',\n",
       " 'like',\n",
       " 'pasta',\n",
       " 'end',\n",
       " 'bbcmtd',\n",
       " 'wholesale',\n",
       " 'markets',\n",
       " 'ablaze',\n",
       " 'http',\n",
       " 'co',\n",
       " 'lhyxeohy6cwe',\n",
       " 'always',\n",
       " 'try',\n",
       " 'bring',\n",
       " 'heavy',\n",
       " 'metal',\n",
       " 'rt',\n",
       " 'http',\n",
       " 'co',\n",
       " 'yao1e0xngw',\n",
       " 'africanbaze',\n",
       " 'breaking',\n",
       " 'news',\n",
       " 'nigeria',\n",
       " 'flag',\n",
       " 'set',\n",
       " 'ablaze',\n",
       " 'aba',\n",
       " 'http',\n",
       " 'co',\n",
       " '2nndbgwyeicrying',\n",
       " 'set',\n",
       " 'ablazeon',\n",
       " 'plus',\n",
       " 'side',\n",
       " 'look',\n",
       " 'sky',\n",
       " 'last',\n",
       " 'night',\n",
       " 'ablaze',\n",
       " 'http',\n",
       " 'co',\n",
       " 'qqsmshaj3n',\n",
       " 'phdsquares',\n",
       " 'mufc',\n",
       " 'built',\n",
       " 'much',\n",
       " 'hype',\n",
       " 'around',\n",
       " 'new',\n",
       " 'acquisitions',\n",
       " 'doubt',\n",
       " 'set',\n",
       " 'epl',\n",
       " 'ablaze',\n",
       " 'season',\n",
       " 'inec',\n",
       " 'office',\n",
       " 'abia',\n",
       " 'set',\n",
       " 'ablaze',\n",
       " 'http',\n",
       " 'co',\n",
       " '3imaomknnabarbados',\n",
       " 'bridgetown',\n",
       " 'jamaica',\n",
       " 'ûò',\n",
       " 'two',\n",
       " 'cars',\n",
       " 'set',\n",
       " 'ablaze',\n",
       " 'santa',\n",
       " 'cruz',\n",
       " 'ûó',\n",
       " 'head',\n",
       " 'st',\n",
       " 'elizabeth',\n",
       " 'police',\n",
       " 'superintende',\n",
       " 'http',\n",
       " 'co',\n",
       " 'wdueaj8q4jablaze',\n",
       " 'lord',\n",
       " 'dcheck',\n",
       " 'http',\n",
       " 'co',\n",
       " 'roi2nsmejj',\n",
       " 'http',\n",
       " 'co',\n",
       " '3tj8zjin21',\n",
       " 'http',\n",
       " 'co',\n",
       " 'yduixefipe',\n",
       " 'http',\n",
       " 'co',\n",
       " 'lxtjc87kls',\n",
       " 'nsfwon',\n",
       " 'outside',\n",
       " 'ablaze',\n",
       " 'alive',\n",
       " 'dead',\n",
       " 'insidehad',\n",
       " 'awesome',\n",
       " 'time',\n",
       " 'visiting',\n",
       " 'cfc',\n",
       " 'head',\n",
       " 'office',\n",
       " 'ancop',\n",
       " 'site',\n",
       " 'ablaze',\n",
       " 'thanks',\n",
       " 'tita',\n",
       " 'vida',\n",
       " 'taking',\n",
       " 'care',\n",
       " 'us',\n",
       " 'soooo',\n",
       " 'pumped',\n",
       " 'ablaze',\n",
       " 'southridgelifei',\n",
       " 'wanted',\n",
       " 'set',\n",
       " 'chicago',\n",
       " 'ablaze',\n",
       " 'preaching',\n",
       " 'hotel',\n",
       " 'http',\n",
       " 'co',\n",
       " 'o9qknbfofxi',\n",
       " 'gained',\n",
       " '3',\n",
       " 'followers',\n",
       " 'last',\n",
       " 'week',\n",
       " 'know',\n",
       " 'stats',\n",
       " 'grow',\n",
       " 'http',\n",
       " 'co',\n",
       " 'tiyulif5c6how',\n",
       " 'west',\n",
       " 'burned',\n",
       " 'thousands',\n",
       " 'wildfires',\n",
       " 'ablaze',\n",
       " 'california',\n",
       " 'alone',\n",
       " 'http',\n",
       " 'co',\n",
       " 'vl5tbr3wbrbuilding',\n",
       " 'perfect',\n",
       " 'tracklist',\n",
       " 'life',\n",
       " 'leave',\n",
       " 'streets',\n",
       " 'ablazefirst',\n",
       " 'night',\n",
       " 'retainers',\n",
       " 'quite',\n",
       " 'weird',\n",
       " 'better',\n",
       " 'get',\n",
       " 'used',\n",
       " 'wear',\n",
       " 'every',\n",
       " 'single',\n",
       " 'night',\n",
       " 'next',\n",
       " 'year',\n",
       " 'least',\n",
       " 'deputies',\n",
       " 'man',\n",
       " 'shot',\n",
       " 'brighton',\n",
       " 'home',\n",
       " 'set',\n",
       " 'ablaze',\n",
       " 'http',\n",
       " 'co',\n",
       " 'gwnrhmso8kman',\n",
       " 'wife',\n",
       " 'get',\n",
       " 'six',\n",
       " 'years',\n",
       " 'jail',\n",
       " 'setting',\n",
       " 'ablaze',\n",
       " 'niece',\n",
       " 'http',\n",
       " 'co',\n",
       " 'ev1ahouczasanta',\n",
       " 'cruz',\n",
       " 'ûó',\n",
       " 'head',\n",
       " 'st',\n",
       " 'elizabeth',\n",
       " 'police',\n",
       " 'superintendent',\n",
       " 'lanford',\n",
       " 'salmon',\n",
       " 'r',\n",
       " 'http',\n",
       " 'co',\n",
       " 'vplr5hka2u',\n",
       " 'http',\n",
       " 'co',\n",
       " 'sxhw2tnnlfpolice',\n",
       " 'arsonist',\n",
       " 'deliberately',\n",
       " 'set',\n",
       " 'black',\n",
       " 'church',\n",
       " 'north',\n",
       " 'carolinaåêablaze',\n",
       " 'http',\n",
       " 'co',\n",
       " 'pcxarbh9annoches',\n",
       " 'el',\n",
       " 'bestia',\n",
       " 'alexis_sanchez',\n",
       " 'happy',\n",
       " 'see',\n",
       " 'teammates',\n",
       " 'training',\n",
       " 'hard',\n",
       " 'goodnight',\n",
       " 'gunners',\n",
       " 'http',\n",
       " 'co',\n",
       " 'uc4j4jhvgr',\n",
       " 'kurds',\n",
       " 'trampling',\n",
       " 'turkmen',\n",
       " 'flag',\n",
       " 'later',\n",
       " 'set',\n",
       " 'ablaze',\n",
       " 'others',\n",
       " 'vandalized',\n",
       " 'offices',\n",
       " 'turkmen',\n",
       " 'front',\n",
       " 'diyala',\n",
       " 'http',\n",
       " 'co',\n",
       " '4izfdyc3cgtruck',\n",
       " 'ablaze',\n",
       " 'r21',\n",
       " 'voortrekker',\n",
       " 'ave',\n",
       " 'outside',\n",
       " 'tambo',\n",
       " 'intl',\n",
       " 'cargo',\n",
       " 'section',\n",
       " 'http',\n",
       " 'co',\n",
       " '8kscqkfkkfset',\n",
       " 'hearts',\n",
       " 'ablaze',\n",
       " 'every',\n",
       " 'city',\n",
       " 'gift',\n",
       " 'every',\n",
       " 'skyline',\n",
       " 'like',\n",
       " 'kiss',\n",
       " 'upon',\n",
       " 'lips',\n",
       " 'û_',\n",
       " 'https',\n",
       " 'co',\n",
       " 'cyompz1a0zthey',\n",
       " 'sky',\n",
       " 'ablaze',\n",
       " 'tonight',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'expecting',\n",
       " 'ig',\n",
       " 'fb',\n",
       " 'filled',\n",
       " 'sunset',\n",
       " 'shots',\n",
       " 'know',\n",
       " 'peeps',\n",
       " 'west',\n",
       " 'burned',\n",
       " 'thousands',\n",
       " 'wildfires',\n",
       " 'ablaze',\n",
       " 'california',\n",
       " 'alone',\n",
       " 'http',\n",
       " 'co',\n",
       " 'icsjgz9te1',\n",
       " 'climate',\n",
       " 'energy',\n",
       " 'http',\n",
       " 'co',\n",
       " '9fxmn0l0bdrevel',\n",
       " 'wmv',\n",
       " 'videos',\n",
       " 'means',\n",
       " 'mac',\n",
       " 'farewell',\n",
       " 'ablaze',\n",
       " 'wmv',\n",
       " 'en',\n",
       " 'route',\n",
       " 'dvd',\n",
       " 'gtxrwmprogressive',\n",
       " 'greetings',\n",
       " 'month',\n",
       " 'students',\n",
       " 'would',\n",
       " 'set',\n",
       " 'pens',\n",
       " 'ablaze',\n",
       " 'torch',\n",
       " 'publications',\n",
       " 'http',\n",
       " 'co',\n",
       " '9fxpixqujtrene',\n",
       " 'ablaze',\n",
       " 'amp',\n",
       " 'jacinta',\n",
       " 'secret',\n",
       " '2k13',\n",
       " 'fallen',\n",
       " 'skies',\n",
       " 'edit',\n",
       " 'mar',\n",
       " '30',\n",
       " '2013',\n",
       " 'https',\n",
       " 'co',\n",
       " '7mlmsuzv1z',\n",
       " 'navista7',\n",
       " 'steve',\n",
       " 'fires',\n",
       " 'something',\n",
       " 'else',\n",
       " 'california',\n",
       " 'tinderbox',\n",
       " 'clown',\n",
       " 'setting',\n",
       " 'hood',\n",
       " 'ablaze',\n",
       " 'news24680',\n",
       " 'nowplaying',\n",
       " 'rene',\n",
       " 'ablaze',\n",
       " 'amp',\n",
       " 'ian',\n",
       " 'buff',\n",
       " 'magnitude',\n",
       " 'http',\n",
       " 'co',\n",
       " 'av2jsjfftc',\n",
       " 'edm',\n",
       " 'nxwestmidlands',\n",
       " 'huge',\n",
       " 'fire',\n",
       " 'wholesale',\n",
       " 'markets',\n",
       " 'ablaze',\n",
       " 'http',\n",
       " 'co',\n",
       " 'rwzbfvnxer',\n",
       " 'ablaze',\n",
       " 'time',\n",
       " 'talk',\n",
       " 'go',\n",
       " 'know',\n",
       " 'make',\n",
       " 'due',\n",
       " 'work',\n",
       " 'kids',\n",
       " 'cuz',\n",
       " 'got',\n",
       " 'bicycle',\n",
       " 'accident',\n",
       " 'amp',\n",
       " 'split',\n",
       " 'testicles',\n",
       " 'impossible',\n",
       " 'kids',\n",
       " 'michael',\n",
       " 'fatheraccident',\n",
       " '24',\n",
       " 'w',\n",
       " 'nashvilletraffic',\n",
       " 'traffic',\n",
       " 'moving',\n",
       " '8m',\n",
       " 'slower',\n",
       " 'usual',\n",
       " 'https',\n",
       " 'co',\n",
       " '0ghk693egjaccident',\n",
       " 'center',\n",
       " 'lane',\n",
       " 'blocked',\n",
       " 'santaclara',\n",
       " 'us',\n",
       " '101',\n",
       " 'nb',\n",
       " 'great',\n",
       " 'america',\n",
       " 'pkwy',\n",
       " 'bayarea',\n",
       " 'traffic',\n",
       " 'http',\n",
       " 'co',\n",
       " 'pmlohzurwrhttp',\n",
       " 'co',\n",
       " 'gkye6gjtk5',\n",
       " 'personalinjury',\n",
       " 'accident',\n",
       " 'summer',\n",
       " 'read',\n",
       " 'advice',\n",
       " 'amp',\n",
       " 'see',\n",
       " 'solicitor',\n",
       " 'help',\n",
       " 'otleyhour',\n",
       " 'stlouis',\n",
       " 'caraccidentlawyer',\n",
       " 'speeding',\n",
       " 'among',\n",
       " 'top',\n",
       " 'causes',\n",
       " 'teen',\n",
       " 'accidents',\n",
       " 'https',\n",
       " 'co',\n",
       " 'k4zomof319',\n",
       " 'https',\n",
       " 'co',\n",
       " 's2kxvm0cba',\n",
       " 'car',\n",
       " 'accident',\n",
       " 'tee',\n",
       " 'û_reported',\n",
       " 'motor',\n",
       " 'vehicle',\n",
       " 'accident',\n",
       " 'curry',\n",
       " 'herman',\n",
       " 'rd',\n",
       " 'near',\n",
       " 'stephenson',\n",
       " 'involving',\n",
       " 'overturned',\n",
       " 'vehicle',\n",
       " 'please',\n",
       " 'use',\n",
       " 'http',\n",
       " 'co',\n",
       " 'ybjezkurw1bigrigradio',\n",
       " 'live',\n",
       " 'accident',\n",
       " 'awarenessi',\n",
       " '77',\n",
       " 'mile',\n",
       " 'marker',\n",
       " '31',\n",
       " 'south',\n",
       " 'mooresville',\n",
       " 'iredell',\n",
       " 'vehicle',\n",
       " 'accident',\n",
       " 'ramp',\n",
       " 'closed',\n",
       " '8',\n",
       " '6',\n",
       " '1',\n",
       " '18',\n",
       " 'pmrt',\n",
       " 'sleepjunkies',\n",
       " 'sleeping',\n",
       " 'pills',\n",
       " 'double',\n",
       " 'risk',\n",
       " 'car',\n",
       " 'accident',\n",
       " 'http',\n",
       " 'co',\n",
       " '7s9nm1fict',\n",
       " 'accident',\n",
       " 'knew',\n",
       " 'gon',\n",
       " 'happen',\n",
       " 'https',\n",
       " 'co',\n",
       " 'ysxun5vcehtraffic',\n",
       " 'accident',\n",
       " 'n',\n",
       " 'cabrillo',\n",
       " 'hwy',\n",
       " 'magellan',\n",
       " 'av',\n",
       " 'mir',\n",
       " '08',\n",
       " '06',\n",
       " '15',\n",
       " '11',\n",
       " '03',\n",
       " '58',\n",
       " '77',\n",
       " 'mile',\n",
       " 'marker',\n",
       " '31',\n",
       " '40',\n",
       " 'south',\n",
       " 'mooresville',\n",
       " 'iredell',\n",
       " 'vehicle',\n",
       " 'accident',\n",
       " 'congestion',\n",
       " '8',\n",
       " '6',\n",
       " '1',\n",
       " '18',\n",
       " 'pmthe',\n",
       " 'pastor',\n",
       " 'scene',\n",
       " 'accident',\n",
       " 'owner',\n",
       " 'range',\n",
       " 'rover',\n",
       " 'mom',\n",
       " 'get',\n",
       " 'home',\n",
       " 'fast',\n",
       " 'wished',\n",
       " 'mom',\n",
       " 'accident',\n",
       " 'truck',\n",
       " 'spilt',\n",
       " 'mayonnaise',\n",
       " 'horrible',\n",
       " 'car',\n",
       " 'accident',\n",
       " 'past',\n",
       " 'sunday',\n",
       " 'finally',\n",
       " 'able',\n",
       " 'get',\n",
       " 'around',\n",
       " 'thank',\n",
       " 'god',\n",
       " 'wait',\n",
       " 'see',\n",
       " 'pissed',\n",
       " 'donnie',\n",
       " 'tell',\n",
       " 'another',\n",
       " 'accident',\n",
       " 'truckcrash',\n",
       " 'overturns',\n",
       " 'fortworth',\n",
       " 'interstate',\n",
       " 'http',\n",
       " 'co',\n",
       " 'rs22lj4qfp',\n",
       " 'click',\n",
       " 'crash',\n",
       " 'gt',\n",
       " 'http',\n",
       " 'co',\n",
       " 'ld0uniyw4kaccident',\n",
       " 'ashville',\n",
       " 'us',\n",
       " '23',\n",
       " 'sb',\n",
       " 'sr',\n",
       " '752',\n",
       " 'traffic',\n",
       " 'http',\n",
       " 'co',\n",
       " 'hylmo0wgficarolina',\n",
       " 'accident',\n",
       " 'motorcyclist',\n",
       " 'dies',\n",
       " '540',\n",
       " 'crash',\n",
       " 'car',\n",
       " 'crossed',\n",
       " 'median',\n",
       " 'motorcycle',\n",
       " 'rider',\n",
       " 'traveling',\n",
       " 'http',\n",
       " 'co',\n",
       " 'p18lzrlmy6fyi',\n",
       " 'cad',\n",
       " 'fyi',\n",
       " 'accident',\n",
       " 'property',\n",
       " 'damage',\n",
       " 'nhs',\n",
       " '999',\n",
       " 'piner',\n",
       " 'rd',\n",
       " 'horndale',\n",
       " 'drrt',\n",
       " 'naayf',\n",
       " 'first',\n",
       " 'accident',\n",
       " 'years',\n",
       " 'turning',\n",
       " 'onto',\n",
       " 'chandanee',\n",
       " 'magu',\n",
       " 'near',\n",
       " 'mma',\n",
       " 'taxi',\n",
       " 'rammed',\n",
       " 'halfway',\n",
       " 'turned',\n",
       " 'everyone',\n",
       " 'conf',\n",
       " 'û_accident',\n",
       " 'left',\n",
       " 'lane',\n",
       " 'blocked',\n",
       " 'manchester',\n",
       " 'rt',\n",
       " '293',\n",
       " 'nb',\n",
       " 'eddy',\n",
       " 'rd',\n",
       " 'stop',\n",
       " 'go',\n",
       " 'traffic',\n",
       " 'back',\n",
       " 'nh',\n",
       " '3a',\n",
       " 'delay',\n",
       " '4',\n",
       " 'mins',\n",
       " 'traffic',\n",
       " 'accident',\n",
       " 'property',\n",
       " 'damage',\n",
       " 'piner',\n",
       " 'rd',\n",
       " 'horndale',\n",
       " 'dr',\n",
       " 'accident',\n",
       " 'http',\n",
       " 'co',\n",
       " 'oia5fxi4gmfyi',\n",
       " 'cad',\n",
       " 'fyi',\n",
       " 'accident',\n",
       " 'property',\n",
       " 'damage',\n",
       " 'wpd',\n",
       " '1600',\n",
       " '17th',\n",
       " 'st8',\n",
       " '6',\n",
       " '2015',\n",
       " '2',\n",
       " '09',\n",
       " 'pm',\n",
       " 'traffic',\n",
       " 'accident',\n",
       " 'injury',\n",
       " '2781',\n",
       " 'willis',\n",
       " 'foreman',\n",
       " 'rd',\n",
       " 'http',\n",
       " 'co',\n",
       " 'vckit6edevaashiqui',\n",
       " 'actress',\n",
       " 'anu',\n",
       " 'aggarwal',\n",
       " 'near',\n",
       " 'fatal',\n",
       " 'accident',\n",
       " 'http',\n",
       " 'co',\n",
       " '6otfp31lqwsuffield',\n",
       " 'alberta',\n",
       " 'accident',\n",
       " 'https',\n",
       " 'co',\n",
       " 'bptmlf4p109',\n",
       " 'mile',\n",
       " 'backup',\n",
       " '77',\n",
       " 'south',\n",
       " 'accident',\n",
       " 'blocking',\n",
       " 'right',\n",
       " '2',\n",
       " 'lanes',\n",
       " 'exit',\n",
       " '31',\n",
       " 'langtree',\n",
       " 'rd',\n",
       " 'consider',\n",
       " 'nc',\n",
       " '115',\n",
       " 'nc',\n",
       " '150',\n",
       " 'nc',\n",
       " '16',\n",
       " 'alternatehas',\n",
       " 'accident',\n",
       " 'changed',\n",
       " 'life',\n",
       " 'help',\n",
       " 'determine',\n",
       " 'options',\n",
       " 'financially',\n",
       " 'support',\n",
       " 'life',\n",
       " 'care',\n",
       " 'plans',\n",
       " 'going',\n",
       " 'treatment',\n",
       " 'breaking',\n",
       " 'deadly',\n",
       " 'motorcycle',\n",
       " 'car',\n",
       " 'accident',\n",
       " 'happened',\n",
       " 'hagerstown',\n",
       " 'today',\n",
       " 'details',\n",
       " '5',\n",
       " 'your4state',\n",
       " 'whag',\n",
       " 'flowri',\n",
       " 'marinading',\n",
       " 'accident',\n",
       " 'car',\n",
       " 'even',\n",
       " 'week',\n",
       " 'got',\n",
       " 'fucking',\n",
       " 'car',\n",
       " 'accident',\n",
       " 'mfs',\n",
       " 'fucking',\n",
       " 'drive',\n",
       " 'norwaymfa',\n",
       " 'bahrain',\n",
       " 'police',\n",
       " 'previously',\n",
       " 'died',\n",
       " 'road',\n",
       " 'accident',\n",
       " 'killed',\n",
       " 'explosion',\n",
       " 'https',\n",
       " 'co',\n",
       " 'gfjfgtodadi',\n",
       " 'still',\n",
       " 'heard',\n",
       " 'church',\n",
       " 'leaders',\n",
       " 'kenya',\n",
       " 'coming',\n",
       " 'forward',\n",
       " 'comment',\n",
       " 'accident',\n",
       " 'issue',\n",
       " 'disciplinary',\n",
       " 'measures',\n",
       " 'arrestpastornganga',\n",
       " 'aftershock_delo',\n",
       " 'scuf',\n",
       " 'ps',\n",
       " 'live',\n",
       " 'game',\n",
       " 'cya',\n",
       " 'man',\n",
       " 'drive',\n",
       " 'effort',\n",
       " 'gets',\n",
       " 'painful',\n",
       " 'man',\n",
       " 'win',\n",
       " 'roger',\n",
       " 'bannister320',\n",
       " 'ir',\n",
       " 'icemoon',\n",
       " 'aftershock',\n",
       " 'http',\n",
       " 'co',\n",
       " 'ynxnvvkcda',\n",
       " 'djicemoon',\n",
       " 'dubstep',\n",
       " 'trapmusic',\n",
       " 'dnb',\n",
       " 'edm',\n",
       " 'dance',\n",
       " 'ices',\n",
       " 'û_',\n",
       " 'http',\n",
       " 'co',\n",
       " 'weqpesenku',\n",
       " 'victory',\n",
       " 'bargain',\n",
       " 'basement',\n",
       " 'prices',\n",
       " 'dwight',\n",
       " 'david',\n",
       " 'eisenhower320',\n",
       " 'ir',\n",
       " 'icemoon',\n",
       " 'aftershock',\n",
       " 'http',\n",
       " 'co',\n",
       " 'vam5podgyw',\n",
       " 'djicemoon',\n",
       " 'dubstep',\n",
       " 'trapmusic',\n",
       " 'dnb',\n",
       " 'edm',\n",
       " 'dance',\n",
       " 'ices',\n",
       " 'û_',\n",
       " 'http',\n",
       " 'co',\n",
       " 'zevakjapcz',\n",
       " 'nobody',\n",
       " 'remembers',\n",
       " 'came',\n",
       " 'second',\n",
       " 'charles',\n",
       " 'schulz',\n",
       " 'aftershock_delo',\n",
       " 'im',\n",
       " 'speaking',\n",
       " 'someone',\n",
       " 'using',\n",
       " 'scuf',\n",
       " 'xb1',\n",
       " 'people',\n",
       " 'end',\n",
       " 'getting',\n",
       " 'ps',\n",
       " 'also',\n",
       " 'harder',\n",
       " 'conflict',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the corpus\n",
    "\n",
    "corpus = process_text_1(raw_corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "facd87a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:41.541414Z",
     "start_time": "2022-06-28T21:02:41.528570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22438"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3202abe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:41.805902Z",
     "start_time": "2022-06-28T21:02:41.762612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co                  4703\n",
       "http                4231\n",
       "https                405\n",
       "amp                  342\n",
       "like                 341\n",
       "                    ... \n",
       "destructiontruck       1\n",
       "salvages               1\n",
       "7b2wf6ovfk             1\n",
       "newsrepublican         1\n",
       "ymy4rskq3d             1\n",
       "Length: 22438, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value counts\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a19ff84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:41.938671Z",
     "start_time": "2022-06-28T21:02:41.935654Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "# sns.barplot(x=tmp.index, y=tmp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33e385b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:42.994678Z",
     "start_time": "2022-06-28T21:02:42.985373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co            4703\n",
       "http          4231\n",
       "https          405\n",
       "amp            342\n",
       "like           341\n",
       "û_             289\n",
       "fire           230\n",
       "get            226\n",
       "via            216\n",
       "2              204\n",
       "people         189\n",
       "new            183\n",
       "one            181\n",
       "news           166\n",
       "emergency      145\n",
       "disaster       143\n",
       "video          136\n",
       "would          133\n",
       "body           127\n",
       "police         122\n",
       "still          120\n",
       "3              119\n",
       "u              117\n",
       "crash          117\n",
       "us             115\n",
       "storm          114\n",
       "back           113\n",
       "day            112\n",
       "know           112\n",
       "california     110\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30st most common tokens\n",
    "\n",
    "tmp.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41d809e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:02:43.137215Z",
     "start_time": "2022-06-28T21:02:43.131163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tydxauuenqhow          1\n",
       "developer              1\n",
       "hld5xlywbncrackdown    1\n",
       "lmwkjsycgj             1\n",
       "danhrothschild         1\n",
       "greed                  1\n",
       "takecare               1\n",
       "cinla1964              1\n",
       "windowgatribble        1\n",
       "contrasts              1\n",
       "foreboding             1\n",
       "expansive              1\n",
       "divisions              1\n",
       "saturation             1\n",
       "hue                    1\n",
       "qbmcsjavt0fall         1\n",
       "homebuyer              1\n",
       "miscalculation         1\n",
       "mwjcdkthere            1\n",
       "workspace              1\n",
       "forsee                 1\n",
       "badkitty               1\n",
       "lt3dave                1\n",
       "specs                  1\n",
       "lore                   1\n",
       "destructiontruck       1\n",
       "salvages               1\n",
       "7b2wf6ovfk             1\n",
       "newsrepublican         1\n",
       "ymy4rskq3d             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30st last common tokens\n",
    "\n",
    "tmp.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "547c0079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:50:52.301633Z",
     "start_time": "2022-06-28T20:50:52.289341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22438.000000\n",
       "mean         3.680096\n",
       "std         43.379216\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          2.000000\n",
       "max       4703.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a9a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.357723Z",
     "start_time": "2022-06-28T20:45:05.357706Z"
    }
   },
   "outputs": [],
   "source": [
    "# sns.displot(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c76df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.358840Z",
     "start_time": "2022-06-28T20:45:05.358827Z"
    }
   },
   "outputs": [],
   "source": [
    "# sns.boxplot(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15905e9c",
   "metadata": {},
   "source": [
    "## 3.2 List rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaca7018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:34.995619Z",
     "start_time": "2022-06-28T21:11:34.944985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dub                   1\n",
       "mxhrextrkh            1\n",
       "ctijdpxabkdogs        1\n",
       "splatling             1\n",
       "foothill              1\n",
       "designsso             1\n",
       "thatrussianman        1\n",
       "waterboarding         1\n",
       "writingtips           1\n",
       "salmanmydarling       1\n",
       "ps3                   1\n",
       "xboxhttps             1\n",
       "qr1l2jyuez            1\n",
       "nester                1\n",
       "switching             1\n",
       "dipping               1\n",
       "pantherattackthere    1\n",
       "dieanpink95           1\n",
       "limitsabe             1\n",
       "yu_nita99             1\n",
       "sivan                 1\n",
       "pantherattacki        1\n",
       "camilla_33            1\n",
       "uooygbb6az            1\n",
       "akq4rwjfvlcheck       1\n",
       "skippy6gaming         1\n",
       "slttorrlhswho         1\n",
       "craykain              1\n",
       "lavalet               1\n",
       "basalt                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words --> not usefull\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_unique_words = tmp[tmp==1]\n",
    "list_unique_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c7ccc02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:35.600280Z",
     "start_time": "2022-06-28T21:11:35.593083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16230"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2eb9b8bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:35.965268Z",
     "start_time": "2022-06-28T21:11:35.955047Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dub',\n",
       " 'mxhrextrkh',\n",
       " 'ctijdpxabkdogs',\n",
       " 'splatling',\n",
       " 'foothill',\n",
       " 'designsso',\n",
       " 'thatrussianman',\n",
       " 'waterboarding',\n",
       " 'writingtips',\n",
       " 'salmanmydarling',\n",
       " 'ps3',\n",
       " 'xboxhttps',\n",
       " 'qr1l2jyuez',\n",
       " 'nester',\n",
       " 'switching',\n",
       " 'dipping',\n",
       " 'pantherattackthere',\n",
       " 'dieanpink95',\n",
       " 'limitsabe',\n",
       " 'yu_nita99',\n",
       " 'sivan',\n",
       " 'pantherattacki',\n",
       " 'camilla_33',\n",
       " 'uooygbb6az',\n",
       " 'akq4rwjfvlcheck',\n",
       " 'skippy6gaming',\n",
       " 'slttorrlhswho',\n",
       " 'craykain',\n",
       " 'lavalet',\n",
       " 'basalt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words = list(list_unique_words.index)\n",
    "list_unique_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b76a7568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:36.280153Z",
     "start_time": "2022-06-28T21:11:36.245549Z"
    }
   },
   "outputs": [],
   "source": [
    "# save it for later\n",
    "\n",
    "tmp = pd.DataFrame({\"words\" : list_unique_words})\n",
    "tmp.to_csv(\"data/cleaned/unique_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07325ab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:36.507075Z",
     "start_time": "2022-06-28T21:11:36.474499Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "motorcycle     5\n",
       "blind          5\n",
       "ices           5\n",
       "remain         5\n",
       "md             5\n",
       "mental         5\n",
       "loves          5\n",
       "depth          5\n",
       "extra          5\n",
       "leaves         5\n",
       "subs           5\n",
       "judge          5\n",
       "earners        5\n",
       "operations     5\n",
       "reduced        5\n",
       "catch          5\n",
       "stephen        5\n",
       "quest          5\n",
       "reviews        5\n",
       "responsible    5\n",
       "motor          5\n",
       "flying         5\n",
       "smithsonian    5\n",
       "52             5\n",
       "34             5\n",
       "losses         5\n",
       "desires        5\n",
       "pulls          5\n",
       "mood           5\n",
       "tubestrike     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_5_words = tmp[tmp<=5]\n",
    "list_min_5_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f8e6b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:36.689521Z",
     "start_time": "2022-06-28T21:11:36.682965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20275"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e8600ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:36.870772Z",
     "start_time": "2022-06-28T21:11:36.839422Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "able           10\n",
       "trees          10\n",
       "complete       10\n",
       "udhampur       10\n",
       "seattle        10\n",
       "word           10\n",
       "michael        10\n",
       "yyc            10\n",
       "amazon         10\n",
       "grows          10\n",
       "jeb            10\n",
       "afghanistan    10\n",
       "picture        10\n",
       "abandoned      10\n",
       "ice            10\n",
       "main           10\n",
       "emotional      10\n",
       "sit            10\n",
       "colour         10\n",
       "nice           10\n",
       "tent           10\n",
       "extreme        10\n",
       "lmao           10\n",
       "ii             10\n",
       "loved          10\n",
       "seeks          10\n",
       "extremely      10\n",
       "issue          10\n",
       "either         10\n",
       "incident       10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_10_words = tmp[tmp<=10]\n",
    "list_min_10_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddcd474b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:36.989147Z",
     "start_time": "2022-06-28T21:11:36.985022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21158"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7328e285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:37.191441Z",
     "start_time": "2022-06-28T21:11:37.152201Z"
    }
   },
   "outputs": [],
   "source": [
    "list_min_10_words = list(list_min_10_words.index)\n",
    "tmp = pd.DataFrame({\"words\" : list_min_10_words})\n",
    "tmp.to_csv(\"data/cleaned/min_10_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f9a94",
   "metadata": {},
   "source": [
    "## 3.3 2nd Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fb47469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:37.861356Z",
     "start_time": "2022-06-28T21:11:37.853105Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_2(doc, \n",
    "                   rejoin=False, \n",
    "                   list_rare_words=None, \n",
    "                   min_len_word=3,\n",
    "                   force_is_alpha=True) : \n",
    "    \"\"\"cf process_text_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "    \n",
    "    positional arguments : \n",
    "    -----------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "    \n",
    "    opt args : \n",
    "    -----------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : the minimum length of words to not exclude\n",
    "    force_is_alpha : int : if 1, exclude all tokens with a numeric character\n",
    "    \n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # classics stopwords\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # no more len words\n",
    "    more_than_N =  [w for w in non_rare_tokens if len(w) >= min_len_word  ]\n",
    "    \n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else :\n",
    "        alpha_tokens = more_than_N\n",
    "    \n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "205a7e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:38.175154Z",
     "start_time": "2022-06-28T21:11:38.157005Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73555            poses\n",
       "15604             http\n",
       "72983               ar\n",
       "37751               im\n",
       "60177             http\n",
       "74331            movie\n",
       "66272        literally\n",
       "39889         fatality\n",
       "35225            fully\n",
       "67158         hakogaku\n",
       "61846    thedailybeast\n",
       "69915         offshoot\n",
       "17760               co\n",
       "75013             back\n",
       "76977           player\n",
       "8887             blood\n",
       "60890            three\n",
       "28283            loose\n",
       "55098             done\n",
       "49290        shipwreck\n",
       "31766          migrant\n",
       "17099            cliff\n",
       "17267              tax\n",
       "11638            years\n",
       "27593             free\n",
       "14047    hksbmijqz1the\n",
       "63950           rubble\n",
       "76384             http\n",
       "55482         disaster\n",
       "57141     obliteration\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(corpus).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f08cac1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:11:38.372060Z",
     "start_time": "2022-06-28T21:11:38.360987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22438"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0441adca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:12:13.542210Z",
     "start_time": "2022-06-28T21:11:39.397840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15618          ass\n",
       "5429         began\n",
       "46730        wanna\n",
       "11444     chemical\n",
       "29903      service\n",
       "36959       barack\n",
       "34411         data\n",
       "46785         onto\n",
       "29520       blames\n",
       "36166     murderer\n",
       "54571     solitude\n",
       "6275     landscape\n",
       "34324      extreme\n",
       "16929          sat\n",
       "45595         sure\n",
       "55663       debate\n",
       "50772     released\n",
       "52779      landing\n",
       "18731      destroy\n",
       "53294      provide\n",
       "35126          bed\n",
       "51968     southern\n",
       "51042    survivors\n",
       "24368     evacuate\n",
       "6054         blood\n",
       "36939         wait\n",
       "53989    whirlwind\n",
       "31874         drop\n",
       "55244      gunfire\n",
       "2998         calif\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_text_2(raw_corpus, list_rare_words=list_unique_words, rejoin=False)\n",
    "pd.Series(corpus).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0d204c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:12:13.559693Z",
     "start_time": "2022-06-28T21:12:13.546792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5705"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb2573",
   "metadata": {},
   "source": [
    "## 3.4 Stem and Lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c87f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c05bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f96896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b12375",
   "metadata": {},
   "source": [
    "## 3.5 3rd cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44aaffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "906a48b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:16:47.888138Z",
     "start_time": "2022-06-28T21:16:47.872999Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_3(doc, \n",
    "                   rejoin=False, \n",
    "                   lemm_or_stemm=\"stem\",\n",
    "                   list_rare_words=None, \n",
    "                   min_len_word=3,\n",
    "                   force_is_alpha=True) : \n",
    "    \"\"\"cf process_text_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "    \n",
    "    positional arguments : \n",
    "    -----------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "    \n",
    "    opt args : \n",
    "    -----------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    lemm_or_stemm : str : if lem do lemmentize else stemmentize  \n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : the minimum length of words to not exclude\n",
    "    force_is_alpha : int : if 1, exclude all tokens with a numeric character\n",
    "    \n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # classics stopwords\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # no more len words\n",
    "    more_than_N =  [w for w in non_rare_tokens if len(w) >= min_len_word  ]\n",
    "    \n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else :\n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_tokens ]\n",
    "    else : \n",
    "        trans = PorterStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens ]\n",
    "        \n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(trans_text)\n",
    "    \n",
    "    return trans_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7400fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:20.991399Z",
     "start_time": "2022-06-28T21:16:48.204435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34563        much\n",
       "23134        much\n",
       "18076       accid\n",
       "21768        year\n",
       "1181     frontlin\n",
       "29056     myanmar\n",
       "11994        came\n",
       "35844       worst\n",
       "24902         ûïa\n",
       "24820       evacu\n",
       "44506        hell\n",
       "13519        bear\n",
       "44816        http\n",
       "40752        http\n",
       "5143         give\n",
       "40073        long\n",
       "40190      stress\n",
       "52354       dixon\n",
       "2803         http\n",
       "54656        year\n",
       "20387    starbuck\n",
       "32042        dijk\n",
       "3152         miss\n",
       "41880        http\n",
       "35482        news\n",
       "49757     thunder\n",
       "547         right\n",
       "19620       senso\n",
       "39781        bout\n",
       "4845        write\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_text_3(raw_corpus, rejoin=False, list_rare_words=list_unique_words)\n",
    "pd.Series(corpus).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28768381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:21.006313Z",
     "start_time": "2022-06-28T21:17:20.994961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4420"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c8be691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:21.154407Z",
     "start_time": "2022-06-28T21:17:21.009315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74211           functionalize\n",
       "143269               perruche\n",
       "6345                   amazed\n",
       "16870                   axled\n",
       "13385                arnberry\n",
       "143098              permeable\n",
       "93718     individualistically\n",
       "196251               sybarist\n",
       "162692         radiotelephone\n",
       "137092                 pajock\n",
       "89533            hyperkinetic\n",
       "14221                  Asilus\n",
       "186016     spectrobolographic\n",
       "80644                gudesire\n",
       "83266                 hebetic\n",
       "101107             kanephoros\n",
       "44589              courthouse\n",
       "63182         epanisognathous\n",
       "96666       interprotoplasmic\n",
       "114701                  metad\n",
       "27279                  bulbar\n",
       "142138              perceiver\n",
       "182970            sleepwalker\n",
       "58807               dynamotor\n",
       "45669             Cristivomer\n",
       "17502           bacteriophage\n",
       "104489            laryngotome\n",
       "129940                   odso\n",
       "7991             anchoretical\n",
       "4343                ailantine\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series( words.words()).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca323c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f275fb7",
   "metadata": {},
   "source": [
    "## 3.5 Only english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7511e15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:21.336634Z",
     "start_time": "2022-06-28T21:17:21.158535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235892"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6572bab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:21.525806Z",
     "start_time": "2022-06-28T21:17:21.340147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'aaron',\n",
       " 'aaronic',\n",
       " 'aaronical',\n",
       " 'aaronite',\n",
       " 'aaronitic',\n",
       " 'aaru',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'ababdeh',\n",
       " 'ababua',\n",
       " 'abac',\n",
       " 'abaca',\n",
       " 'abacate',\n",
       " 'abacay',\n",
       " 'abacinate',\n",
       " 'abacination',\n",
       " 'abaciscus',\n",
       " 'abacist',\n",
       " 'aback',\n",
       " 'abactinal',\n",
       " 'abactinally',\n",
       " 'abaction',\n",
       " 'abactor',\n",
       " 'abaculus',\n",
       " 'abacus',\n",
       " 'abadite',\n",
       " 'abaff',\n",
       " 'abaft',\n",
       " 'abaisance',\n",
       " 'abaiser',\n",
       " 'abaissed',\n",
       " 'abalienate',\n",
       " 'abalienation',\n",
       " 'abalone',\n",
       " 'abama',\n",
       " 'abampere',\n",
       " 'abandon',\n",
       " 'abandonable',\n",
       " 'abandoned',\n",
       " 'abandonedly',\n",
       " 'abandonee',\n",
       " 'abandoner',\n",
       " 'abandonment',\n",
       " 'abanic',\n",
       " 'abantes',\n",
       " 'abaptiston',\n",
       " 'abarambo',\n",
       " 'abaris',\n",
       " 'abarthrosis',\n",
       " 'abarticular',\n",
       " 'abarticulation',\n",
       " 'abas',\n",
       " 'abase',\n",
       " 'abased',\n",
       " 'abasedly',\n",
       " 'abasedness',\n",
       " 'abasement',\n",
       " 'abaser',\n",
       " 'abasgi',\n",
       " 'abash',\n",
       " 'abashed',\n",
       " 'abashedly',\n",
       " 'abashedness',\n",
       " 'abashless',\n",
       " 'abashlessly',\n",
       " 'abashment',\n",
       " 'abasia',\n",
       " 'abasic',\n",
       " 'abask',\n",
       " 'abassin',\n",
       " 'abastardize',\n",
       " 'abatable',\n",
       " 'abate',\n",
       " 'abatement',\n",
       " 'abater',\n",
       " 'abatis',\n",
       " 'abatised',\n",
       " 'abaton',\n",
       " 'abator',\n",
       " 'abattoir',\n",
       " 'abatua',\n",
       " 'abature',\n",
       " 'abave',\n",
       " 'abaxial',\n",
       " 'abaxile',\n",
       " 'abaze',\n",
       " 'abb',\n",
       " 'abba',\n",
       " 'abbacomes',\n",
       " 'abbacy',\n",
       " 'abbadide',\n",
       " 'abbas',\n",
       " 'abbasi',\n",
       " 'abbassi',\n",
       " 'abbasside',\n",
       " 'abbatial',\n",
       " 'abbatical',\n",
       " 'abbess',\n",
       " 'abbey',\n",
       " 'abbeystede',\n",
       " 'abbie',\n",
       " 'abbot',\n",
       " 'abbotcy',\n",
       " 'abbotnullius',\n",
       " 'abbotship',\n",
       " 'abbreviate',\n",
       " 'abbreviately',\n",
       " 'abbreviation',\n",
       " 'abbreviator',\n",
       " 'abbreviatory',\n",
       " 'abbreviature',\n",
       " 'abby',\n",
       " 'abcoulomb',\n",
       " 'abdal',\n",
       " 'abdat',\n",
       " 'abderian',\n",
       " 'abderite',\n",
       " 'abdest',\n",
       " 'abdicable',\n",
       " 'abdicant',\n",
       " 'abdicate',\n",
       " 'abdication',\n",
       " 'abdicative',\n",
       " 'abdicator',\n",
       " 'abdiel',\n",
       " 'abditive',\n",
       " 'abditory',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abdominales',\n",
       " 'abdominalian',\n",
       " 'abdominally',\n",
       " 'abdominoanterior',\n",
       " 'abdominocardiac',\n",
       " 'abdominocentesis',\n",
       " 'abdominocystic',\n",
       " 'abdominogenital',\n",
       " 'abdominohysterectomy',\n",
       " 'abdominohysterotomy',\n",
       " 'abdominoposterior',\n",
       " 'abdominoscope',\n",
       " 'abdominoscopy',\n",
       " 'abdominothoracic',\n",
       " 'abdominous',\n",
       " 'abdominovaginal',\n",
       " 'abdominovesical',\n",
       " 'abduce',\n",
       " 'abducens',\n",
       " 'abducent',\n",
       " 'abduct',\n",
       " 'abduction',\n",
       " 'abductor',\n",
       " 'abe',\n",
       " 'abeam',\n",
       " 'abear',\n",
       " 'abearance',\n",
       " 'abecedarian',\n",
       " 'abecedarium',\n",
       " 'abecedary',\n",
       " 'abed',\n",
       " 'abeigh',\n",
       " 'abel',\n",
       " 'abele',\n",
       " 'abelia',\n",
       " 'abelian',\n",
       " 'abelicea',\n",
       " 'abelite',\n",
       " 'abelite',\n",
       " 'abelmoschus',\n",
       " 'abelmosk',\n",
       " 'abelonian',\n",
       " 'abeltree',\n",
       " 'abencerrages',\n",
       " 'abenteric',\n",
       " 'abepithymia',\n",
       " 'aberdeen',\n",
       " 'aberdevine',\n",
       " 'aberdonian',\n",
       " 'aberia',\n",
       " 'aberrance',\n",
       " 'aberrancy',\n",
       " 'aberrant',\n",
       " 'aberrate',\n",
       " 'aberration',\n",
       " 'aberrational',\n",
       " 'aberrator',\n",
       " 'aberrometer',\n",
       " 'aberroscope',\n",
       " 'aberuncator',\n",
       " 'abet',\n",
       " 'abetment',\n",
       " 'abettal',\n",
       " 'abettor',\n",
       " 'abevacuation',\n",
       " 'abey',\n",
       " 'abeyance',\n",
       " 'abeyancy',\n",
       " 'abeyant',\n",
       " 'abfarad',\n",
       " 'abhenry',\n",
       " 'abhiseka',\n",
       " 'abhominable',\n",
       " 'abhor',\n",
       " 'abhorrence',\n",
       " 'abhorrency',\n",
       " 'abhorrent',\n",
       " 'abhorrently',\n",
       " 'abhorrer',\n",
       " 'abhorrible',\n",
       " 'abhorring',\n",
       " 'abhorson',\n",
       " 'abidal',\n",
       " 'abidance',\n",
       " 'abide',\n",
       " 'abider',\n",
       " 'abidi',\n",
       " 'abiding',\n",
       " 'abidingly',\n",
       " 'abidingness',\n",
       " 'abie',\n",
       " 'abies',\n",
       " 'abietate',\n",
       " 'abietene',\n",
       " 'abietic',\n",
       " 'abietin',\n",
       " 'abietineae',\n",
       " 'abietineous',\n",
       " 'abietinic',\n",
       " 'abiezer',\n",
       " 'abigail',\n",
       " 'abigail',\n",
       " 'abigailship',\n",
       " 'abigeat',\n",
       " 'abigeus',\n",
       " 'abilao',\n",
       " 'ability',\n",
       " 'abilla',\n",
       " 'abilo',\n",
       " 'abintestate',\n",
       " 'abiogenesis',\n",
       " 'abiogenesist',\n",
       " 'abiogenetic',\n",
       " 'abiogenetical',\n",
       " 'abiogenetically',\n",
       " 'abiogenist',\n",
       " 'abiogenous',\n",
       " 'abiogeny',\n",
       " 'abiological',\n",
       " 'abiologically',\n",
       " 'abiology',\n",
       " 'abiosis',\n",
       " 'abiotic',\n",
       " 'abiotrophic',\n",
       " 'abiotrophy',\n",
       " 'abipon',\n",
       " 'abir',\n",
       " 'abirritant',\n",
       " 'abirritate',\n",
       " 'abirritation',\n",
       " 'abirritative',\n",
       " 'abiston',\n",
       " 'abitibi',\n",
       " 'abiuret',\n",
       " 'abject',\n",
       " 'abjectedness',\n",
       " 'abjection',\n",
       " 'abjective',\n",
       " 'abjectly',\n",
       " 'abjectness',\n",
       " 'abjoint',\n",
       " 'abjudge',\n",
       " 'abjudicate',\n",
       " 'abjudication',\n",
       " 'abjunction',\n",
       " 'abjunctive',\n",
       " 'abjuration',\n",
       " 'abjuratory',\n",
       " 'abjure',\n",
       " 'abjurement',\n",
       " 'abjurer',\n",
       " 'abkar',\n",
       " 'abkari',\n",
       " 'abkhas',\n",
       " 'abkhasian',\n",
       " 'ablach',\n",
       " 'ablactate',\n",
       " 'ablactation',\n",
       " 'ablare',\n",
       " 'ablastemic',\n",
       " 'ablastous',\n",
       " 'ablate',\n",
       " 'ablation',\n",
       " 'ablatitious',\n",
       " 'ablatival',\n",
       " 'ablative',\n",
       " 'ablator',\n",
       " 'ablaut',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ableeze',\n",
       " 'ablegate',\n",
       " 'ableness',\n",
       " 'ablepharia',\n",
       " 'ablepharon',\n",
       " 'ablepharous',\n",
       " 'ablepharus',\n",
       " 'ablepsia',\n",
       " 'ableptical',\n",
       " 'ableptically',\n",
       " 'abler',\n",
       " 'ablest',\n",
       " 'ablewhackets',\n",
       " 'ablins',\n",
       " 'abloom',\n",
       " 'ablow',\n",
       " 'ablude',\n",
       " 'abluent',\n",
       " 'ablush',\n",
       " 'ablution',\n",
       " 'ablutionary',\n",
       " 'abluvion',\n",
       " 'ably',\n",
       " 'abmho',\n",
       " 'abnaki',\n",
       " 'abnegate',\n",
       " 'abnegation',\n",
       " 'abnegative',\n",
       " 'abnegator',\n",
       " 'abner',\n",
       " 'abnerval',\n",
       " 'abnet',\n",
       " 'abneural',\n",
       " 'abnormal',\n",
       " 'abnormalism',\n",
       " 'abnormalist',\n",
       " 'abnormality',\n",
       " 'abnormalize',\n",
       " 'abnormally',\n",
       " 'abnormalness',\n",
       " 'abnormity',\n",
       " 'abnormous',\n",
       " 'abnumerable',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'abobra',\n",
       " 'abode',\n",
       " 'abodement',\n",
       " 'abody',\n",
       " 'abohm',\n",
       " 'aboil',\n",
       " 'abolish',\n",
       " 'abolisher',\n",
       " 'abolishment',\n",
       " 'abolition',\n",
       " 'abolitionary',\n",
       " 'abolitionism',\n",
       " 'abolitionist',\n",
       " 'abolitionize',\n",
       " 'abolla',\n",
       " 'aboma',\n",
       " 'abomasum',\n",
       " 'abomasus',\n",
       " 'abominable',\n",
       " 'abominableness',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abominator',\n",
       " 'abomine',\n",
       " 'abongo',\n",
       " 'aboon',\n",
       " 'aborad',\n",
       " 'aboral',\n",
       " 'aborally',\n",
       " 'abord',\n",
       " 'aboriginal',\n",
       " 'aboriginality',\n",
       " 'aboriginally',\n",
       " 'aboriginary',\n",
       " 'aborigine',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborticide',\n",
       " 'abortient',\n",
       " 'abortifacient',\n",
       " 'abortin',\n",
       " 'abortion',\n",
       " 'abortional',\n",
       " 'abortionist',\n",
       " 'abortive',\n",
       " 'abortively',\n",
       " 'abortiveness',\n",
       " 'abortus',\n",
       " 'abouchement',\n",
       " 'abound',\n",
       " 'abounder',\n",
       " 'abounding',\n",
       " 'aboundingly',\n",
       " 'about',\n",
       " 'abouts',\n",
       " 'above',\n",
       " 'aboveboard',\n",
       " 'abovedeck',\n",
       " 'aboveground',\n",
       " 'aboveproof',\n",
       " 'abovestairs',\n",
       " 'abox',\n",
       " 'abracadabra',\n",
       " 'abrachia',\n",
       " 'abradant',\n",
       " 'abrade',\n",
       " 'abrader',\n",
       " 'abraham',\n",
       " 'abrahamic',\n",
       " 'abrahamidae',\n",
       " 'abrahamite',\n",
       " 'abrahamitic',\n",
       " 'abraid',\n",
       " 'abram',\n",
       " 'abramis',\n",
       " 'abranchial',\n",
       " 'abranchialism',\n",
       " 'abranchian',\n",
       " 'abranchiata',\n",
       " 'abranchiate',\n",
       " 'abranchious',\n",
       " 'abrasax',\n",
       " 'abrase',\n",
       " 'abrash',\n",
       " 'abrasiometer',\n",
       " 'abrasion',\n",
       " 'abrasive',\n",
       " 'abrastol',\n",
       " 'abraum',\n",
       " 'abraxas',\n",
       " 'abreact',\n",
       " 'abreaction',\n",
       " 'abreast',\n",
       " 'abrenounce',\n",
       " 'abret',\n",
       " 'abrico',\n",
       " 'abridge',\n",
       " 'abridgeable',\n",
       " 'abridged',\n",
       " 'abridgedly',\n",
       " 'abridger',\n",
       " 'abridgment',\n",
       " 'abrim',\n",
       " 'abrin',\n",
       " 'abristle',\n",
       " 'abroach',\n",
       " 'abroad',\n",
       " 'abrocoma',\n",
       " 'abrocome',\n",
       " 'abrogable',\n",
       " 'abrogate',\n",
       " 'abrogation',\n",
       " 'abrogative',\n",
       " 'abrogator',\n",
       " 'abroma',\n",
       " 'abronia',\n",
       " 'abrook',\n",
       " 'abrotanum',\n",
       " 'abrotine',\n",
       " 'abrupt',\n",
       " 'abruptedly',\n",
       " 'abruption',\n",
       " 'abruptly',\n",
       " 'abruptness',\n",
       " 'abrus',\n",
       " 'absalom',\n",
       " 'absampere',\n",
       " 'absaroka',\n",
       " 'absarokite',\n",
       " 'abscess',\n",
       " 'abscessed',\n",
       " 'abscession',\n",
       " 'abscessroot',\n",
       " 'abscind',\n",
       " 'abscise',\n",
       " 'abscision',\n",
       " 'absciss',\n",
       " 'abscissa',\n",
       " 'abscissae',\n",
       " 'abscisse',\n",
       " 'abscission',\n",
       " 'absconce',\n",
       " 'abscond',\n",
       " 'absconded',\n",
       " 'abscondedly',\n",
       " 'abscondence',\n",
       " 'absconder',\n",
       " 'absconsa',\n",
       " 'abscoulomb',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absentation',\n",
       " 'absentee',\n",
       " 'absenteeism',\n",
       " 'absenteeship',\n",
       " 'absenter',\n",
       " 'absently',\n",
       " 'absentment',\n",
       " 'absentmindedly',\n",
       " 'absentness',\n",
       " 'absfarad',\n",
       " 'abshenry',\n",
       " 'absi',\n",
       " 'absinthe',\n",
       " 'absinthial',\n",
       " 'absinthian',\n",
       " 'absinthiate',\n",
       " 'absinthic',\n",
       " 'absinthin',\n",
       " 'absinthine',\n",
       " 'absinthism',\n",
       " 'absinthismic',\n",
       " 'absinthium',\n",
       " 'absinthol',\n",
       " 'absit',\n",
       " 'absmho',\n",
       " 'absohm',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absoluteness',\n",
       " 'absolution',\n",
       " 'absolutism',\n",
       " 'absolutist',\n",
       " 'absolutistic',\n",
       " 'absolutistically',\n",
       " 'absolutive',\n",
       " 'absolutization',\n",
       " 'absolutize',\n",
       " 'absolutory',\n",
       " 'absolvable',\n",
       " 'absolvatory',\n",
       " 'absolve',\n",
       " 'absolvent',\n",
       " 'absolver',\n",
       " 'absolvitor',\n",
       " 'absolvitory',\n",
       " 'absonant',\n",
       " 'absonous',\n",
       " 'absorb',\n",
       " 'absorbability',\n",
       " 'absorbable',\n",
       " 'absorbed',\n",
       " 'absorbedly',\n",
       " 'absorbedness',\n",
       " 'absorbefacient',\n",
       " 'absorbency',\n",
       " 'absorbent',\n",
       " 'absorber',\n",
       " 'absorbing',\n",
       " 'absorbingly',\n",
       " 'absorbition',\n",
       " 'absorpt',\n",
       " 'absorptance',\n",
       " 'absorptiometer',\n",
       " 'absorptiometric',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'absorptively',\n",
       " 'absorptiveness',\n",
       " 'absorptivity',\n",
       " 'absquatulate',\n",
       " 'abstain',\n",
       " 'abstainer',\n",
       " 'abstainment',\n",
       " 'abstemious',\n",
       " 'abstemiously',\n",
       " 'abstemiousness',\n",
       " 'abstention',\n",
       " 'abstentionist',\n",
       " 'abstentious',\n",
       " 'absterge',\n",
       " 'abstergent',\n",
       " 'abstersion',\n",
       " 'abstersive',\n",
       " 'abstersiveness',\n",
       " 'abstinence',\n",
       " 'abstinency',\n",
       " 'abstinent',\n",
       " 'abstinential',\n",
       " 'abstinently',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'abstractedly',\n",
       " 'abstractedness',\n",
       " 'abstracter',\n",
       " 'abstraction',\n",
       " 'abstractional',\n",
       " 'abstractionism',\n",
       " 'abstractionist',\n",
       " 'abstractitious',\n",
       " 'abstractive',\n",
       " 'abstractively',\n",
       " 'abstractiveness',\n",
       " 'abstractly',\n",
       " 'abstractness',\n",
       " 'abstractor',\n",
       " 'abstrahent',\n",
       " 'abstricted',\n",
       " 'abstriction',\n",
       " 'abstruse',\n",
       " 'abstrusely',\n",
       " 'abstruseness',\n",
       " 'abstrusion',\n",
       " 'abstrusity',\n",
       " 'absume',\n",
       " 'absumption',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'absvolt',\n",
       " 'absyrtus',\n",
       " 'abterminal',\n",
       " 'abthain',\n",
       " 'abthainrie',\n",
       " 'abthainry',\n",
       " 'abthanage',\n",
       " 'abu',\n",
       " 'abu',\n",
       " 'abucco',\n",
       " 'abulia',\n",
       " 'abulic',\n",
       " 'abulomania',\n",
       " 'abuna',\n",
       " 'abundance',\n",
       " 'abundancy',\n",
       " 'abundant',\n",
       " 'abundantia',\n",
       " 'abundantly',\n",
       " 'abura',\n",
       " 'aburabozu',\n",
       " 'aburban',\n",
       " 'aburst',\n",
       " 'aburton',\n",
       " 'abusable',\n",
       " 'abuse',\n",
       " 'abusedly',\n",
       " 'abusee',\n",
       " 'abuseful',\n",
       " 'abusefully',\n",
       " 'abusefulness',\n",
       " 'abuser',\n",
       " 'abusion',\n",
       " 'abusious',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abusiveness',\n",
       " 'abut',\n",
       " 'abuta',\n",
       " 'abutilon',\n",
       " 'abutment',\n",
       " 'abuttal',\n",
       " 'abutter',\n",
       " 'abutting',\n",
       " 'abuzz',\n",
       " 'abvolt',\n",
       " 'abwab',\n",
       " 'aby',\n",
       " 'abysm',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'abyssal',\n",
       " 'abyssinian',\n",
       " 'abyssobenthonic',\n",
       " 'abyssolith',\n",
       " 'abyssopelagic',\n",
       " 'acacatechin',\n",
       " 'acacatechol',\n",
       " 'acacetin',\n",
       " 'acacia',\n",
       " 'acacian',\n",
       " 'acaciin',\n",
       " 'acacin',\n",
       " 'academe',\n",
       " 'academial',\n",
       " 'academian',\n",
       " 'academic',\n",
       " 'academic',\n",
       " 'academical',\n",
       " 'academically',\n",
       " 'academicals',\n",
       " 'academician',\n",
       " 'academicism',\n",
       " 'academism',\n",
       " 'academist',\n",
       " 'academite',\n",
       " 'academization',\n",
       " 'academize',\n",
       " 'academus',\n",
       " 'academy',\n",
       " 'acadia',\n",
       " 'acadialite',\n",
       " 'acadian',\n",
       " 'acadie',\n",
       " 'acaena',\n",
       " 'acajou',\n",
       " 'acaleph',\n",
       " 'acalepha',\n",
       " 'acalephae',\n",
       " 'acalephan',\n",
       " 'acalephoid',\n",
       " 'acalycal',\n",
       " 'acalycine',\n",
       " 'acalycinous',\n",
       " 'acalyculate',\n",
       " 'acalypha',\n",
       " 'acalypterae',\n",
       " 'acalyptrata',\n",
       " 'acalyptratae',\n",
       " 'acalyptrate',\n",
       " 'acamar',\n",
       " 'acampsia',\n",
       " 'acana',\n",
       " 'acanaceous',\n",
       " 'acanonical',\n",
       " 'acanth',\n",
       " 'acantha',\n",
       " 'acanthaceae',\n",
       " 'acanthaceous',\n",
       " 'acanthad',\n",
       " 'acantharia',\n",
       " 'acanthia',\n",
       " 'acanthial',\n",
       " 'acanthin',\n",
       " 'acanthine',\n",
       " 'acanthion',\n",
       " 'acanthite',\n",
       " 'acanthocarpous',\n",
       " 'acanthocephala',\n",
       " 'acanthocephalan',\n",
       " 'acanthocephali',\n",
       " 'acanthocephalous',\n",
       " 'acanthocereus',\n",
       " 'acanthocladous',\n",
       " 'acanthodea',\n",
       " 'acanthodean',\n",
       " 'acanthodei',\n",
       " 'acanthodes',\n",
       " 'acanthodian',\n",
       " 'acanthodidae',\n",
       " 'acanthodii',\n",
       " 'acanthodini',\n",
       " 'acanthoid',\n",
       " 'acantholimon',\n",
       " 'acanthological',\n",
       " 'acanthology',\n",
       " 'acantholysis',\n",
       " 'acanthoma',\n",
       " 'acanthomeridae',\n",
       " 'acanthon',\n",
       " 'acanthopanax',\n",
       " 'acanthophis',\n",
       " 'acanthophorous',\n",
       " 'acanthopod',\n",
       " 'acanthopodous',\n",
       " 'acanthopomatous',\n",
       " 'acanthopore',\n",
       " 'acanthopteran',\n",
       " 'acanthopteri',\n",
       " 'acanthopterous',\n",
       " 'acanthopterygian',\n",
       " 'acanthopterygii',\n",
       " 'acanthosis',\n",
       " 'acanthous',\n",
       " 'acanthuridae',\n",
       " 'acanthurus',\n",
       " 'acanthus',\n",
       " 'acapnia',\n",
       " 'acapnial',\n",
       " 'acapsular',\n",
       " 'acapu',\n",
       " 'acapulco',\n",
       " 'acara',\n",
       " 'acarapis',\n",
       " 'acardia',\n",
       " 'acardiac',\n",
       " 'acari',\n",
       " 'acarian',\n",
       " 'acariasis',\n",
       " 'acaricidal',\n",
       " 'acaricide',\n",
       " 'acarid',\n",
       " 'acarida',\n",
       " 'acaridea',\n",
       " 'acaridean',\n",
       " 'acaridomatium',\n",
       " 'acariform',\n",
       " 'acarina',\n",
       " 'acarine',\n",
       " 'acarinosis',\n",
       " 'acarocecidium',\n",
       " 'acarodermatitis',\n",
       " 'acaroid',\n",
       " 'acarol',\n",
       " 'acarologist',\n",
       " 'acarology',\n",
       " 'acarophilous',\n",
       " 'acarophobia',\n",
       " 'acarotoxic',\n",
       " 'acarpelous',\n",
       " 'acarpous',\n",
       " 'acarus',\n",
       " 'acastus',\n",
       " 'acatalectic',\n",
       " 'acatalepsia',\n",
       " 'acatalepsy',\n",
       " 'acataleptic',\n",
       " 'acatallactic',\n",
       " 'acatamathesia',\n",
       " 'acataphasia',\n",
       " 'acataposis',\n",
       " 'acatastasia',\n",
       " 'acatastatic',\n",
       " 'acate',\n",
       " 'acategorical',\n",
       " 'acatery',\n",
       " 'acatharsia',\n",
       " 'acatharsy',\n",
       " 'acatholic',\n",
       " 'acaudal',\n",
       " 'acaudate',\n",
       " 'acaulescent',\n",
       " 'acauline',\n",
       " 'acaulose',\n",
       " 'acaulous',\n",
       " 'acca',\n",
       " 'accede',\n",
       " 'accedence',\n",
       " 'acceder',\n",
       " 'accelerable',\n",
       " 'accelerando',\n",
       " 'accelerant',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'acceleratedly',\n",
       " 'acceleration',\n",
       " 'accelerative',\n",
       " 'accelerator',\n",
       " 'acceleratory',\n",
       " 'accelerograph',\n",
       " 'accelerometer',\n",
       " 'accend',\n",
       " 'accendibility',\n",
       " 'accendible',\n",
       " 'accension',\n",
       " 'accensor',\n",
       " 'accent',\n",
       " 'accentless',\n",
       " 'accentor',\n",
       " 'accentuable',\n",
       " 'accentual',\n",
       " 'accentuality',\n",
       " 'accentually',\n",
       " 'accentuate',\n",
       " 'accentuation',\n",
       " 'accentuator',\n",
       " 'accentus',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptableness',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'acceptancy',\n",
       " 'acceptant',\n",
       " 'acceptation',\n",
       " 'accepted',\n",
       " 'acceptedly',\n",
       " 'accepter',\n",
       " 'acceptilate',\n",
       " 'acceptilation',\n",
       " 'acception',\n",
       " 'acceptive',\n",
       " 'acceptor',\n",
       " 'acceptress',\n",
       " 'accerse',\n",
       " 'accersition',\n",
       " 'accersitor',\n",
       " 'access',\n",
       " 'accessarily',\n",
       " 'accessariness',\n",
       " 'accessary',\n",
       " 'accessaryship',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessibly',\n",
       " 'accession',\n",
       " 'accessional',\n",
       " 'accessioner',\n",
       " 'accessive',\n",
       " 'accessively',\n",
       " 'accessless',\n",
       " 'accessorial',\n",
       " 'accessorily',\n",
       " 'accessoriness',\n",
       " 'accessorius',\n",
       " 'accessory',\n",
       " 'accidence',\n",
       " 'accidency',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentalism',\n",
       " 'accidentalist',\n",
       " 'accidentality',\n",
       " 'accidentally',\n",
       " 'accidentalness',\n",
       " 'accidented',\n",
       " 'accidential',\n",
       " 'accidentiality',\n",
       " 'accidently',\n",
       " 'accidia',\n",
       " 'accidie',\n",
       " 'accinge',\n",
       " 'accipient',\n",
       " 'accipiter',\n",
       " 'accipitral',\n",
       " 'accipitrary',\n",
       " 'accipitres',\n",
       " 'accipitrine',\n",
       " 'accismus',\n",
       " 'accite',\n",
       " 'acclaim',\n",
       " 'acclaimable',\n",
       " 'acclaimer',\n",
       " 'acclamation',\n",
       " 'acclamator',\n",
       " 'acclamatory',\n",
       " 'acclimatable',\n",
       " 'acclimatation',\n",
       " 'acclimate',\n",
       " 'acclimatement',\n",
       " 'acclimation',\n",
       " 'acclimatizable',\n",
       " 'acclimatization',\n",
       " 'acclimatize',\n",
       " 'acclimatizer',\n",
       " 'acclimature',\n",
       " 'acclinal',\n",
       " 'acclinate',\n",
       " 'acclivitous',\n",
       " 'acclivity',\n",
       " 'acclivous',\n",
       " 'accloy',\n",
       " 'accoast',\n",
       " 'accoil',\n",
       " 'accolade',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accolent',\n",
       " 'accolle',\n",
       " 'accombination',\n",
       " 'accommodable',\n",
       " 'accommodableness',\n",
       " 'accommodate',\n",
       " 'accommodately',\n",
       " 'accommodateness',\n",
       " 'accommodating',\n",
       " 'accommodatingly',\n",
       " 'accommodation',\n",
       " 'accommodational',\n",
       " 'accommodative',\n",
       " 'accommodativeness',\n",
       " 'accommodator',\n",
       " 'accompanier',\n",
       " 'accompaniment',\n",
       " 'accompanimental',\n",
       " 'accompanist',\n",
       " 'accompany',\n",
       " 'accompanyist',\n",
       " 'accompletive',\n",
       " 'accomplice',\n",
       " 'accompliceship',\n",
       " 'accomplicity',\n",
       " 'accomplish',\n",
       " 'accomplishable',\n",
       " 'accomplished',\n",
       " 'accomplisher',\n",
       " 'accomplishment',\n",
       " 'accomplisht',\n",
       " 'accompt',\n",
       " 'accord',\n",
       " 'accordable',\n",
       " 'accordance',\n",
       " 'accordancy',\n",
       " 'accordant',\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_words = [i.lower() for i in words.words()]\n",
    "eng_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a8675c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:21.596280Z",
     "start_time": "2022-06-28T21:17:21.530387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234377"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(eng_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3bc7455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:28.453319Z",
     "start_time": "2022-06-28T21:17:21.599325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109607              macrandr\n",
       "68000                   fake\n",
       "233636             wreakless\n",
       "155595            priestshir\n",
       "154971           presignific\n",
       "40093               commonli\n",
       "214780              undertel\n",
       "223078               unsolid\n",
       "158327         prudentialist\n",
       "166309               rejuven\n",
       "122140                  naga\n",
       "67248                 extern\n",
       "32811                cercele\n",
       "160068               punctur\n",
       "89530          hyperkeratosi\n",
       "6626             ametabolian\n",
       "141291                peixer\n",
       "69665                fidelio\n",
       "77973     glossolabiopharyng\n",
       "40603                concaus\n",
       "2820            adrenalectom\n",
       "60793               elotillo\n",
       "208547                truthi\n",
       "113152            megalopsia\n",
       "70643              flangeway\n",
       "21746                  bhaga\n",
       "72415                foresin\n",
       "182018            siphonogam\n",
       "15229                atmolyz\n",
       "83822             hemalbumen\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "eng_words_stem = [ps.stem(i) for i in eng_words]\n",
    "pd.Series(eng_words_stem).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "610242ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:28.518158Z",
     "start_time": "2022-06-28T21:17:28.455981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178311"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(eng_words_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab12e9a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:32.108827Z",
     "start_time": "2022-06-28T21:17:28.521308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116336          milliner\n",
       "219309           unmated\n",
       "8167        androphagous\n",
       "65504            eunomia\n",
       "117414      misprofessor\n",
       "82610         hartmannia\n",
       "215709         unemulous\n",
       "200170    teratoblastoma\n",
       "22476         binominous\n",
       "74150             fumago\n",
       "92746       incatenation\n",
       "169849          ridicule\n",
       "113113      megalocornea\n",
       "134459         overcloud\n",
       "118948        monopteral\n",
       "57801           dregless\n",
       "236615          surprise\n",
       "120008     mountainwards\n",
       "3375          aetosaurus\n",
       "134480    overcompensate\n",
       "45002         craniology\n",
       "222616         unsevered\n",
       "50679         demiourgoi\n",
       "184787           solvend\n",
       "54500      disappreciate\n",
       "160447         pussyfoot\n",
       "32994      certificative\n",
       "75096           gambeson\n",
       "226909           vanadyl\n",
       "174326             scall\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "eng_words_lem = [lm.lemmatize(i) for i in eng_words]\n",
    "pd.Series(eng_words_lem).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f189495d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T21:17:32.119824Z",
     "start_time": "2022-06-28T21:17:32.113255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_words_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d060ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.412228Z",
     "start_time": "2022-06-28T20:45:05.412213Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_4(doc, rejoin=True, lemm_or_stemm = \"stem\", list_rare_words=None, min_len_word=3, eng_words=None) : \n",
    "    \"\"\"df v3 but with only valid english word\"\"\"\n",
    " \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower and strip\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # remove stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    # drop rare tokens\n",
    "    non_rare_tokens_list = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # keep only len word > N\n",
    "    more_than_N =  [w for w in non_rare_tokens_list if len(w) >= 3 ]\n",
    "    \n",
    "    # keep only alpha not num\n",
    "    alpha_num = [w for w in more_than_N if w.isalpha()]\n",
    "    \n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_num ]\n",
    "    else : \n",
    "        trans = PorterStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_num ]\n",
    "        \n",
    "    # in english \n",
    "    if eng_words :\n",
    "        engl_text = [i for i in trans_text if i in eng_words]\n",
    "    else :\n",
    "        engl_text = trans_text\n",
    "        \n",
    "    #  return a list or a string\n",
    "    if rejoin : \n",
    "        return \" \".join(engl_text)\n",
    "    \n",
    "    return engl_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5660a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.415022Z",
     "start_time": "2022-06-28T20:45:05.415004Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = process_text_4(raw_corpus, rejoin=False, list_rare_words=list_unique_words, eng_words=eng_words_stem)\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181a359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.416472Z",
     "start_time": "2022-06-28T20:45:05.416457Z"
    }
   },
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d12c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.417589Z",
     "start_time": "2022-06-28T20:45:05.417575Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afb159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.419239Z",
     "start_time": "2022-06-28T20:45:05.419219Z"
    }
   },
   "outputs": [],
   "source": [
    "list_unique_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f226e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.421140Z",
     "start_time": "2022-06-28T20:45:05.421120Z"
    }
   },
   "outputs": [],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9a61a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.424241Z",
     "start_time": "2022-06-28T20:45:05.424219Z"
    }
   },
   "outputs": [],
   "source": [
    "list_min_5_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccce498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.426002Z",
     "start_time": "2022-06-28T20:45:05.425985Z"
    }
   },
   "outputs": [],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179c6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.427330Z",
     "start_time": "2022-06-28T20:45:05.427313Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = process_text_4(raw_corpus, rejoin=False, list_rare_words=list_min_5_words, eng_words=eng_words_stem)\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.428970Z",
     "start_time": "2022-06-28T20:45:05.428952Z"
    }
   },
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ee4bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.431693Z",
     "start_time": "2022-06-28T20:45:05.431672Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623efe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.433771Z",
     "start_time": "2022-06-28T20:45:05.433752Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sns.barplot(tmp.index, tmp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4abba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.436409Z",
     "start_time": "2022-06-28T20:45:05.436385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', stopwords = [], max_words = 50).generate(\" \".join(corpus))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcda990",
   "metadata": {},
   "source": [
    "## by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12526deb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.438132Z",
     "start_time": "2022-06-28T20:45:05.438110Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1 = df[df.target == 1]\n",
    "df_0 = df[df.target == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fc9b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.439899Z",
     "start_time": "2022-06-28T20:45:05.439878Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_1 = \" \".join(df_1.text)\n",
    "corpus_0 = \" \".join(df_0.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d404ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.441809Z",
     "start_time": "2022-06-28T20:45:05.441786Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_1 = process_text_4(corpus_1, rejoin=False, list_rare_words=list_min_5_words, eng_words=eng_words_stem)\n",
    "corpus_0 = process_text_4(corpus_0, rejoin=False, list_rare_words=list_min_5_words, eng_words=eng_words_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0ebd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T16:06:27.176963Z",
     "start_time": "2022-06-27T16:06:27.174034Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa0c90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.443651Z",
     "start_time": "2022-06-28T20:45:05.443625Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', stopwords = [], max_words = 50).generate(\" \".join(corpus_1))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e774a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.446406Z",
     "start_time": "2022-06-28T20:45:05.446315Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', stopwords = [], max_words = 50).generate(\" \".join(corpus_0))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dcbe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.449969Z",
     "start_time": "2022-06-28T20:45:05.449949Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(corpus_1).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56913590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.451543Z",
     "start_time": "2022-06-28T20:45:05.451522Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(corpus_0).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf262359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.453028Z",
     "start_time": "2022-06-28T20:45:05.453013Z"
    }
   },
   "outputs": [],
   "source": [
    "[i for i in pd.Series(corpus_1).value_counts().head(10).index if i in pd.Series(corpus_0).value_counts().head(10).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66921930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.454676Z",
     "start_time": "2022-06-28T20:45:05.454656Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text_5(doc, rejoin=True, lemm_or_stemm = \"stem\", list_rare_words=None, min_len_word=3, eng_words=None) : \n",
    "    \"\"\"df v4 but exclude amp\"\"\"\n",
    " \n",
    "    # list_unique_words\n",
    "    if not list_rare_words: \n",
    "        list_rare_words = []\n",
    "        \n",
    "    # lower and strip\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # remove stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    # drop rare tokens\n",
    "    non_rare_tokens_list = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # keep only len word > N\n",
    "    more_than_N =  [w for w in non_rare_tokens_list if len(w) >= 3 ]\n",
    "    \n",
    "    # keep only alpha not num\n",
    "    alpha_num = [w for w in more_than_N if w.isalpha()]\n",
    "    \n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_num ]\n",
    "    else : \n",
    "        trans = PorterStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_num ]\n",
    "        \n",
    "    # in english \n",
    "    if eng_words :\n",
    "        engl_text = [i for i in trans_text if i in eng_words]\n",
    "    else :\n",
    "        engl_text = trans_text\n",
    "       \n",
    "    # amp\n",
    "    engl_text = [i for i in engl_text if i!=\"amp\"]\n",
    "        \n",
    "    #  return a list or a string\n",
    "    if rejoin : \n",
    "        return \" \".join(engl_text)\n",
    "    \n",
    "    return engl_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afecb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b24fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.457102Z",
     "start_time": "2022-06-28T20:45:05.457080Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_clean(doc) : \n",
    "    \n",
    "    new_doc = process_text_5(doc,rejoin=True, stem_or_lem=\"lem\", list_rare_words=list_min_5_words, eng_words=eng_words_lem)\n",
    "    return  new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0865a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.458978Z",
     "start_time": "2022-06-28T20:45:05.458963Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df.text.apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71bafb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.460437Z",
     "start_time": "2022-06-28T20:45:05.460422Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.461930Z",
     "start_time": "2022-06-28T20:45:05.461912Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf82f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T20:45:05.464027Z",
     "start_time": "2022-06-28T20:45:05.464011Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"data/cleaned/final_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac914f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
